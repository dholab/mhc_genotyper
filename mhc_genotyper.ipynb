{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyrqsPfO3KNv"
      },
      "source": [
        "# Summary:\n",
        "- This genotyper helps summerize and classify non-human primates in an easy to read .xlsx file.\n",
        "- For MCM and MAMU it can help classify known haploytypes under the correct circumstnaces (low error rate, not alot of read hopping, and matches to a known haplotype).\n",
        "- The algorithm is for classifiying haplotypes is straight forward (does not use machine learning) to make sure we limit false negatives.\n",
        "- This workflow is for files that have NOT been processed through labkey.  \n",
        "- Files must be uploaded to Google Drive first.\n",
        "- Your file path to google drive folder must be declared.\n",
        "- Your samplesheet must be included.\n",
        "- Your sample name will be from the sample sheet.\n",
        "- Your output will be name+datetimestamp\n",
        "## What this does:\n",
        "- Maps the reads to known alleles\n",
        "- uses vsearch to eliminate any chimeras and find unique reads, and get summerized counts of each allele\n",
        "- List the read count for each known allele\n",
        "- Sort the alleles in a predifined logicall order\n",
        "- When it can, call the haplotype\n",
        "- Save the results as an excel sheet.\n",
        "- Save work (reduced size sam files that are grouped by count, saving 98% of diskspace).\n",
        "\n",
        "## How to run it:\n",
        "### Prerequisite:\n",
        "Prior to running this script...\n",
        "1. fastq.gz files must be correctly uploaded to google drive folder:\n",
        "    - Shared drives/dholab/gs/genotyper/input_fq / <fastq_folder_name>\n",
        "2. Your samplesheet.csv must be in the same folder as the fastq.gz files\n",
        "3. If you have multiple .csv files, you must denote the name in the secure text box or in the declare inputs.  If there is only one it will automatically find the sample sheet.\n",
        "    1. Your sample sheet must have the following columns contained: Sample_ID, Sample_Name, Sample_Project, Description\n",
        "    2. Description can be blank, but the other three must be filled out\n",
        "    3. Add a species column to your worksheet (unquoted):\n",
        "        - use 'Pig-tailed' for 'MANE'\n",
        "        - use 'Cynomolgus' for 'MCM'\n",
        "        - use 'Rhesus' for 'MAMU'\n",
        "\n",
        "\n",
        "\n",
        "### Running this script:\n",
        "1. Scroll to \"DECLARE INPUTS\" and in the code block change the following as needed:\n",
        "    - EXPERIMENT = '27251' # experiment number\n",
        "    - folder_name = 'miseq810' # must match the foldername your input fastq.gz reads are stored in google drive  \n",
        "    - project_list = [] # optional must be perfectly spelled and is case sensitve as on your sample sheet\n",
        "    - RUN_ID = '27251' # your run id\n",
        "\n",
        "    - sample_path = None # denote the full path of your sample sheet if there are multiple sample sheets (or .csv files) in the folder, or it is located in a different folder.\n",
        "2. Run all\n",
        "3. If you have not used the script recently, there may be a pop up to connect your googe drive follow the steps.\n",
        "\n",
        "\n",
        "## TROUBLESHOOTING\n",
        "- if you have trouble connecting to a session:\n",
        "1. open a drive.google.com,\n",
        "2. refresh\n",
        "3. authenticated with duo for UW service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc0gN2m-DiJS"
      },
      "source": [
        "# File formats and sample sheet\n",
        "- File name nomenclature:\n",
        "    - Example: MiSeq855_v2/DW428_S98_L001_R1_001.fastq.gz\n",
        "    - Folder_Name, all files + sample sheet must be in this folder\n",
        "    - Only one CSV file (samplesheet) can be in this folder or you must name the sample sheet full path\n",
        "    - [Sample_Name]\\_S[Miseq_Designated_Sample_Number]\\_[LANE]\\_R[1 or 2]_[enumerator].fastq.gz\n",
        "    - The Sample_Name must uniquely match the sample name on the samplesheet\n",
        "- Sample sheet format\n",
        "    - Columns\n",
        "        - Sample_ID :  file number from the Miseq raw name (Output from the automatically ran BCL2FASTQ\n",
        "        - Sample_Name: Our internal Name for the sample/animal\n",
        "        - Sample_Project: Internal Sample project we use in Labkey\n",
        "        - Description: The Customers name for the sample/animal\n",
        "        - Species: Must be the case sensitive species that is in labkey.\n",
        "            - Species allowable:\n",
        "            - use 'Pig-tailed' for 'MANE'\n",
        "            - use 'Cynomolgus' for 'MCM'\n",
        "            - use 'Rhesus' for 'MAMU'\n",
        "## Example Row:\n",
        "```\n",
        "Sample_ID, Sample_Name, Sample_Project, Description, Species\n",
        "74564, DW427, Kenyon19, H20C31, Cynomolgus\n",
        "```\n",
        "## The files nomencalure are symbolically converted to the labkey's nomenclature:\n",
        "- This is the naming convention that we would typically work with and we will convert it to the labkey processed format.\n",
        "- Example: SampleSheet-R1-70123.fastq.gz\n",
        "- [SampleSheet Name]-[R1 or R2]-[Sample_ID].fastq.gz\n",
        "- SampleSheet is the name of the sample sheet it used, (with out the .csv extension)\n",
        "- direction of the read (R1 or R2)\n",
        "- Sample_ID (the file number or Sample_ID, this is a unique id for each sample and is enumerated in labkey)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssanP_Qe3OUd"
      },
      "source": [
        "# Mount Google Drive\n",
        "- The mount should be /content/drive\n",
        "- Follow the pop up to sign into UW (needs duo (re) authentication every 1-7 days)\n",
        "hours\n",
        "- This will hang or time out if you do not follow the prompt and get it mounted\n",
        "- YOU MUST mount it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF5Lofow3jSz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "Shareddrives_path = '/content/drive/Shared drives'\n",
        "MyDrive_path= '/content/drive/MyDrive'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mOFvbKAHl5L"
      },
      "source": [
        "# DECLARE INPUTS\n",
        "- enter a relavent Experiment number\n",
        "- Enter a miseq folder number\n",
        "\n",
        "\n",
        "## if you get an error:\n",
        "- Make sure your API key is correct and valid.\n",
        "- Make sure labkey is not down\n",
        "- escalate with error code if you cannot determine the cause"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P48eF1tQVSvZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "EXPERIMENT = '31728' # experiment number\n",
        "RUN_ID = '31728' # run number (you can just use experiment name is there isn't one)\n",
        "# folder_name ='MiSeq902'\n",
        "folder_name ='MiSeq944' # your folder name from '/content/drive/Shared drives/dholab/gs/genotyper/input_fq/<folder_name>\n",
        "sample_path = None # leave as none unless your you have multiple .csv files in your fastq folder, or your sample sheet is in a different location\n",
        "project_list = ['CPRC20'] # optional must be perfectly spelled and is case sensitve as on your sample sheet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MlJfNNW20fd"
      },
      "source": [
        "# Only use secure textbox if your api key is not stored on google drive. Do not edit code below this line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJeX313X7UAu"
      },
      "outputs": [],
      "source": [
        "############################################################################################################################################\n",
        "####  You should not need to edit below this line (except to enter the api key in the dynamic secret text box below this code section). #####\n",
        "\n",
        "input_folder = os.path.join(Shareddrives_path,'dholab/gs/genotyper/input_fq', folder_name)\n",
        "output_folder = os.path.join(Shareddrives_path,'dholab/gs/genotyper/output')\n",
        "if os.path.exists(input_folder):\n",
        "    print(\"Make sure fastq.gz and samplesheet (.csv) have been successfully copied to google drive dir: \")\n",
        "    print(input_folder)\n",
        "else:\n",
        "    print(\"creat a input folder and copy  fastq.gz and samplesheet (.csv) to it in google drive dir:\")\n",
        "    print(input_folder)\n",
        "    raise('INPUT FOLDER DOES NOT EXIST!')\n",
        "file_list = os.listdir(input_folder)\n",
        "sample_list = [x for x in file_list if x.endswith('.csv')]\n",
        "fastq_list = [x for x in file_list if x.endswith('.fastq.gz') or x.endswith('.fq.gz')]\n",
        "if len(sample_list) == 1:\n",
        "    sample_path = os.path.join(input_folder, sample_list[0])\n",
        "    print('Sample sheet path: {0}'.format(sample_path))\n",
        "elif len(sample_list ==0):\n",
        "    print('NO SAMPLE SHEET FOUND')\n",
        "    raise('INPUT FOLDER DOES NOT EXIST!')\n",
        "else:\n",
        "    if sample_path is None:\n",
        "        raise('TOO many CSV files, declare sample path!')\n",
        "    if not os.path.exists(sample_path):\n",
        "        raise('Sample path declared does not exist, this needs to be a full path!')\n",
        "    print('Sample sheet path: {0}'.format(sample_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQtybjyk2oO-"
      },
      "source": [
        "# PIPELINE BEGINS: DO EDIT BELOW THIS SECTION!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ1DO1zF4JRO"
      },
      "source": [
        "## Output files:\n",
        "- The output files will go /content/drive/Shared drives/dholab/gs/genotyper/output/*miseqnumber*/*cohort_name*/timestamp/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuZpsjeQIKED"
      },
      "source": [
        "# Begin Pipeline:\n",
        "- unless trouble shooting, you should not have to edit below this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs6i_gbkIBCT"
      },
      "source": [
        "## install prerequisites\n",
        "### Genotype miSeq data against reference FASTA\n",
        "\n",
        "This is a new implementation of the MHC genotyping pipeline. Considerations:\n",
        "\n",
        "- Integration with LabKey to reduce manual typing\n",
        "- Speed (condense identical reads pre-genotyping). This should make it more feasible to reanalyze old data with new allele databases. Current throughput is about 360 samples per hour (10 seconds per sample).\n",
        "- Flexible reporting with Pandas (create framework for adding haplotyping) and to support eventual genotyping results in LabKey\n",
        "- Export to Excel similar to current format\n",
        "- Jupyter Notebook for portability and reproducible data analysis. This is really important so we can distribute users' data and the full analysis of their results.\n",
        "\n",
        "One possibly controversial decision in this algorithm is that I selectively include identical sequences that are found  as a fraction of total reads. This runs the risk of losing some sequences that could potentially be informative. When making this decision, I thought a lot about lossless compression of music. There is a lot of discussion about whether lossy compression of music files (e.g., 320kb MP3 is distinguishable from lossless FLAC/ALAC (https://www.npr.org/sections/therecord/2015/06/02/411473508/how-well-can-you-hear-audio-quality). I think there is a parallel in MHC genotyping -- do we really need to know all MHC if they are present in very low abundance of cDNA? Could we improve genotyping by simply reporting those sequences that comprise a significant fraction of reads (set to 0.1% of total reads by default)? I would need to be convinced that this really helps.\n",
        "\n",
        "### Dependencies\n",
        "\n",
        "+ Labkey pip install labkey==2.0.1\n",
        "+ Jupyter Notebook/Jupyter Lab\n",
        "+ Python 3 (tested on anaconda distribution of Python 3.6.4)\n",
        "+ Access to dholk.primate.wisc.edu\n",
        "+ pigz (in PATH)\n",
        "+ bbmap (in PATH)\n",
        "+ bbmerge (in PATH)\n",
        "+ bbduk (in PATH)\n",
        "+  DO NOT USE: USEARCH v10 (discontinued)\n",
        "+ vsearch\n",
        "    + ! wget https://github.com/torognes/vsearch/releases/download/v2.21.1/vsearch-2.21.1-linux-x86_64.tar.gz\n",
        "+ Pandas (tested from anaconda distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_SBsxKBO_7-"
      },
      "source": [
        "### labkey and pigz installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-0unLNWIAP_"
      },
      "outputs": [],
      "source": [
        "# labkey is installed earlier for a better gui experience\n",
        "! apt install pigz\n",
        "! pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dieaTwh03FHj"
      },
      "source": [
        "### Vsearch and BBMAP installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2fizikt3Gwr"
      },
      "outputs": [],
      "source": [
        "! cp '{Shareddrives_path}/dholab/gs/genotyper/vsearch-2.21.1-linux-x86_64.tar.gz' /content\n",
        "! tar -xzf /content/vsearch-2.21.1-linux-x86_64.tar.gz\n",
        "vsearch_path = '/content/vsearch-2.21.1-linux-x86_64/bin/vsearch'\n",
        "USEARCH_PATH = vsearch_path\n",
        "! cp '{Shareddrives_path}/dholab/gs/genotyper/bbmap.tar.gz' /content\n",
        "! tar -xzf /content/bbmap.tar.gz\n",
        "# this might not matter but just realized this is the wrong version.\n",
        "\n",
        "BBDUK_PATH = '/content/bbmap/bbduk.sh'\n",
        "BBMERGE_PATH = '/content/bbmap/bbmerge.sh'\n",
        "BBMAP_PATH = '/content/bbmap/bbmap.sh'\n",
        "BBMAP2_PATH = ['java','-ea', '-Xmx8043m', '-Xms8043m', '-cp','/content/bbmap/current/', 'align2.BBMap', 'build=1']\n",
        "STATS_PATH = '/content/bbmap/stats.sh'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DLXX6kgd3J9"
      },
      "outputs": [],
      "source": [
        "!pip install pysam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8RbERDSNHLG"
      },
      "source": [
        "### import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAER8svJJuMd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ1T2pu0OZ08"
      },
      "outputs": [],
      "source": [
        "! {vsearch_path} -v\n",
        "! {BBMAP_PATH} --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QibDKh2IDvi"
      },
      "source": [
        "## Configure file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh45h5muG4VR"
      },
      "outputs": [],
      "source": [
        "genotyper_root_dir = os.path.join(Shareddrives_path,'dholab/gs/genotyper')\n",
        "\n",
        "ref_dict = {\n",
        "    'REF' : {\n",
        "        'MCM' : os.path.join(genotyper_root_dir, 'ref/2016_MCM_MiSeq_ref/MCM_MHC-all_mRNA-MiSeq_singles-RENAME_20Jun16.fasta'),\n",
        "        'MANE': os.path.join(genotyper_root_dir, 'ref/26128_IPD-MHC_2021-07-09_MiSeq_Refs/Mane_MiSeq-IPD_17.06.01_2.2.0.0_plus_SP_RW.fasta'),\n",
        "        'MAMU':os.path.join(genotyper_root_dir, 'ref/26128_IPD-MHC_2021-07-09_MiSeq_Refs/26128_ipd-mhc-mamu-2021-07-09.miseq.RWv4.fasta')\n",
        "    },\n",
        "    'PCR_PRIMERS': os.path.join(genotyper_root_dir, 'ref/SBT195_MHCII_primers_2Sep13.fasta')\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljP_VMIh5HwG"
      },
      "source": [
        "# Results and cohort sorting intermediate input files structure\n",
        "- The input files will be automatically moved by cohort. This is because the pivot tables are contructed by cohort and this will be one less step.\n",
        "    - they will go to /content/drive/Shared drives/dholab/gs/genotyper/input_fq_by_cohort/*miseqnumber*/*cohort_name*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HB0FoqL3FHo"
      },
      "source": [
        "# split up main dir into different cohorts\n",
        "- open the sample sheet\n",
        "- look split by same cohorts\n",
        "- mkdir for each cohort\n",
        "- mv files to each by cohort\n",
        "- use cyber duck's webdav to download the files from illumina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sejQAwn73FHs"
      },
      "outputs": [],
      "source": [
        "# create nested dictionaries of dictionaries to support haplotyping against multiple haplotype definitions\n",
        "\n",
        "### Indian rhesus ###\n",
        "indian_rhesus = {'PREFIX' : 'Mamu'}\n",
        "\n",
        "# Indian rhesus MHC updated by Roger 24 August 2021\n",
        "\n",
        "indian_rhesus['MHC_A_HAPLOTYPES'] = {\n",
        "'A001.01' : ['A1_001'],\n",
        "'A002.01' : ['A1_002_01'],\n",
        "'A003.01' : ['A1_003'],\n",
        "'A004.01' : ['A1_004'],\n",
        "'A006.01' : ['A1_006'],\n",
        "'A007.01' : ['A1_007'],\n",
        "'A008.01' : ['A1_008'],\n",
        "'A011.01' : ['A1_011'],\n",
        "'A012.01' : ['A1_012'],\n",
        "'A016.01' : ['A1_016'],\n",
        "'A018.01' : ['A1_018'],\n",
        "'A018.02' : ['A1_018', 'A2_01'],\n",
        "'A019.01' : ['A1_019'],\n",
        "'A019.02' : ['A1_019_11', 'A1_003'],\n",
        "'A022.01' : ['A1_022'],\n",
        "'A023.01' : ['A1_023'],\n",
        "'A025.01' : ['A1_025'],\n",
        "'A026.01' : ['A1_026'],\n",
        "'A028.01' : ['A1_028g'],\n",
        "'A055.01' : ['A1_055'],\n",
        "'A074.01' : ['A1_074'],\n",
        "'A110-A111.01' : ['A1_110_A1_111'],\n",
        "'A224.01' : ['A2_24', 'A1_003'],\n",
        "}\n",
        "\n",
        "indian_rhesus['MHC_B_HAPLOTYPES'] = {\n",
        "'B001.01' : ['B_001', 'B_007', 'B_030'],\n",
        "'B001.03' : ['B_001_02', 'B_094', 'B_095'],\n",
        "'B002.01' : ['B_002'],\n",
        "'B008.01' : ['B_008', 'B_006'],\n",
        "'B012.01' : ['B_012', 'B_030', 'B_082'],\n",
        "'B012.02' : ['B_012', 'B_022', 'B_030'],\n",
        "'B012.03' : ['B_012', 'B_022', 'B_030', 'B_031g'],\n",
        "'B015.01' : ['B_015g2', 'B_005g'],\n",
        "'B015.02' : ['B_015g2', 'B_068g1'],\n",
        "'B015.03' : ['B_015g2', 'B_031g', 'B_068g1'],\n",
        "'B017.01' : ['B_017', 'B_029'],\n",
        "'B017.02' : ['B_017', 'B_065', 'B_083'],\n",
        "'B017.04' : ['B_017', 'B_065', 'B_068', 'B_083'],\n",
        "'B024.01' : ['B_024', 'B_019'],\n",
        "'B028.01' : ['B_028', 'B_021'],\n",
        "'B043.01' : ['B_043', 'B_030'],\n",
        "'B043.02' : ['B_043', 'B_030', 'B_031_03', 'B_073'],\n",
        "'B043.03' : ['B_043', 'B_030', 'B_073'],\n",
        "'B045.01' : ['B_045', 'B_037'],\n",
        "'B047.01' : ['B_047'],\n",
        "'B048.01' : ['B_048', 'B_041'],\n",
        "'B055.01' : ['B_055', 'B_052', 'B_058'],\n",
        "'B056.01' : ['B_056', 'B_067'],\n",
        "'B056.02' : ['B_056', 'B_066', 'B_068'],\n",
        "'B069.01' : ['B_069', 'B_065'],\n",
        "'B069.02' : ['B_069', 'B_068', 'B_075'],\n",
        "'B071.01' : ['B_047_B_071', 'B_006'],\n",
        "'B080.01' : ['B_080', 'B_081'],\n",
        "'B091.01' : ['B_091', 'B_068'],\n",
        "'B093.01' : ['B_093'],\n",
        "'B106.01' : ['B_106', 'B_033'],\n",
        "}\n",
        "\n",
        "indian_rhesus['MHC_DRB_HAPLOTYPES'] = {\n",
        "'DR01.01' : ['DRB1_04_06_01', 'DRB5_03_01'],\n",
        "'DR01.03' : ['DRB1_04_11', 'DRB5_03_09'],\n",
        "'DR01.04' : ['DRB1_04_06_01', 'DRB5_03_09'],\n",
        "'DR02.01' : ['DRB3_04_03', 'DRB_W003_05'],\n",
        "'DR03.01' : ['DRB1_03_03_01', 'DRB1_10_07'],\n",
        "'DR03.02' : ['DRB1_03_12', 'DRB1_10_07'],\n",
        "'DR03.03' : ['DRB1_03_17', 'DRB1_10_08'],\n",
        "'DR03.04' : ['DRB1_03_18', 'DRB1_10_03'],\n",
        "'DR03.05' : ['DRB1_03_06', 'DRB1_10_07'],\n",
        "'DR03.06' : ['DRB1_03_06', 'DRB1_10_03'],\n",
        "'DR03.07' : ['DRB1_03_19', 'DRB1_10_03'],\n",
        "'DR03.08' : ['DRB1_03_03_01', 'DRB1_10_03'],\n",
        "'DR03.09' : ['DRB1_03_20', 'DRB1_10_02_02'],\n",
        "'DR04.01' : ['DRB1_03_09', 'DRB_W002_01'],\n",
        "'DR04.02' : ['DRB1_03_18', 'DRB_W002_01'],\n",
        "'DR04.03' : ['DRB1_03_09', 'DRB_W002_03'],\n",
        "'DR05.01' : ['DRB1_04_03', 'DRB_W005_01'],\n",
        "'DR05.02' : ['DRB1_04_03', 'DRB_W005_02'],\n",
        "'DR06.01' : ['DRB_W003_03', 'DRB_W004_01'],\n",
        "'DR08.01' : ['DRB_W028_01', 'DRB3_04_09', 'DRB5_03_07'],\n",
        "'DR09.01' : ['DRB1_04_04', 'DRB_W007_02_01', 'DRB_W003_07'],\n",
        "'DR09.02' : ['DRB1_04_08', 'DRB_W007_01'],\n",
        "'DR10.01' : ['DRB1_07_01', 'DRB3_04_05', 'DRB5_03_03'],\n",
        "'DR10.02' : ['DRB1_07_01', 'DRB3_04_05', 'DRB5_03_01'],\n",
        "'DR11.01' : ['DRB_W025_01'],\n",
        "'DR11.02' : ['DRB_W205_w_01'],\n",
        "'DR11.03' : ['DRB_W025_05', 'DRB1_07_04'],\n",
        "'DR13.01' : ['DRB1_03_18', 'DRB_W006_03', 'DRB_W006_04'],\n",
        "'DR13.02' : ['DRB1_03_18', 'DRB_W006_11', 'DRB_W006_04'],\n",
        "'DR14.01' : ['DRB3_04_10', 'DRB_W004_02', 'DRB_W027_01'],\n",
        "'DR14.02' : ['DRB3_04_10', 'DRB_W004_02', 'DRB_W027_02'],\n",
        "'DR15.01/02' : ['DRB_W006_06', 'DRB_W021_04', 'DRB_W026g'],\n",
        "'DR15.03' : ['DRB_W006_06', 'DRB_W021_04', 'DRB_W002_01'],\n",
        "'DR16.01' : ['DRB1_03_10', 'DRB_W001_01', 'DRB_W006_02', 'DRB_W006_09_01'],\n",
        "'DR18.01' : ['DRB4_01_02', 'DRB5_03_06'],\n",
        "'DR28.01' : ['DRB1_07g', 'DRB4_01_04', 'DRB_W102_01'],\n",
        "'DR29.01' : ['DRB1_10_11', 'DRB_W001_05'],\n",
        "'DR30.01' : ['DRB1_07_05', 'DRB_W002_03']\n",
        "}\n",
        "\n",
        "indian_rhesus['MHC_DQA_HAPLOTYPES'] = {\n",
        "'01_02' : ['DQA1_01_02'],\n",
        "'01_07' : ['DQA1_01_07'],\n",
        "'01_09' : ['DQA1_01_09'],\n",
        "'01g1' : ['DQA1_01g1'],\n",
        "'01g2' : ['DQA1_01g2'],\n",
        "'01g3' : ['DQA1_01g3'],\n",
        "'01g4' : ['DQA1_01g4'],\n",
        "'05_01' : ['DQA1_05_01'],\n",
        "'05_02' : ['DQA1_05_02'],\n",
        "'05_03' : ['DQA1_05_03'],\n",
        "'05_04' : ['DQA1_05_04'],\n",
        "'05_05' : ['DQA1_05_05'],\n",
        "'05_06' : ['DQA1_05_06'],\n",
        "'05_07' : ['DQA1_05_07'],\n",
        "'23_01' : ['DQA1_23_01'],\n",
        "'23_02' : ['DQA1_23_02'],\n",
        "'23_03' : ['DQA1_23_03'],\n",
        "'24_02' : ['DQA1_24_02'],\n",
        "'24_04' : ['DQA1_24_04'],\n",
        "'24_08' : ['DQA1_24_08'],\n",
        "'24g1' : ['DQA1_24g1'],\n",
        "'24g2' : ['DQA1_24g2'],\n",
        "'26_01' : ['DQA1_26_01'],\n",
        "'26g1' : ['DQA1_26g1'],\n",
        "'26g2' : ['DQA1_26g2']\n",
        "}\n",
        "\n",
        "indian_rhesus['MHC_DQB_HAPLOTYPES'] = {\n",
        "'06_01' : ['DQB1_06_01'],\n",
        "'06_07' : ['DQB1_06_07'],\n",
        "'06_08' : ['DQB1_06_08'],\n",
        "'06_09' : ['DQB1_06_09'],\n",
        "'06_10' : ['DQB1_06_10'],\n",
        "'06_13_01' : ['DQB1_06_13_01'],\n",
        "'06g1' : ['DQB1_06g1'],\n",
        "'06g2' : ['DQB1_06g2'],\n",
        "'06g3' : ['DQB1_06g3'],\n",
        "'06g4' : ['DQB1_06g4'],\n",
        "'15_02' : ['DQB1_15_02'],\n",
        "'15g1' : ['DQB1_15g1'],\n",
        "'15g2' : ['DQB1_15g2'],\n",
        "'16_01' : ['DQB1_16_01'],\n",
        "'16_03' : ['DQB1_16_03'],\n",
        "'17_03' : ['DQB1_17_03'],\n",
        "'17g1' : ['DQB1_17g1'],\n",
        "'17g2' : ['DQB1_17g2'],\n",
        "'17g3' : ['DQB1_17g3'],\n",
        "'18_08' : ['DQB1_18_08'],\n",
        "'18_10' : ['DQB1_18_10'],\n",
        "'18_12' : ['DQB1_18_12'],\n",
        "'18_17' : ['DQB1_18_17'],\n",
        "'18_20' : ['DQB1_18_20'],\n",
        "'18_24' : ['DQB1_18_24'],\n",
        "'18g3' : ['DQB1_18g3'],\n",
        "'18g4' : ['DQB1_18g4'],\n",
        "'18g5' : ['DQB1_18g5'],\n",
        "'24_01' : ['DQB1_24_01'],\n",
        "'27g' : ['DQB1_27g']\n",
        "}\n",
        "\n",
        "indian_rhesus['MHC_DPA_HAPLOTYPES'] = {\n",
        "'02_03' : ['DPA1_02_03'],\n",
        "'02_08' : ['DPA1_02_08'],\n",
        "'02_13' : ['DPA1_02_13'],\n",
        "'02_14' : ['DPA1_02_14'],\n",
        "'02_15' : ['DPA1_02_15'],\n",
        "'02_16' : ['DPA1_02_16'],\n",
        "'02_20' : ['DPA1_02_20'],\n",
        "'02g1' : ['DPA1_02g1'],\n",
        "'02g2' : ['DPA1_02g2'],\n",
        "'02g3' : ['DPA1_02g3'],\n",
        "'02g4' : ['DPA1_02g4'],\n",
        "'04_01' : ['DPA1_04_01'],\n",
        "'04_04' : ['DPA1_04_04'],\n",
        "'04g' : ['DPA1_04g'],\n",
        "'06g' : ['DPA1_06g'],\n",
        "'07_01' : ['DPA1_07_01'],\n",
        "'07_04' : ['DPA1_07_04'],\n",
        "'07_09' : ['DPA1_07_09'],\n",
        "'07g1' : ['DPA1_07g1'],\n",
        "'07g2' : ['DPA1_07g2'],\n",
        "'07g3' : ['DPA1_07g3'],\n",
        "'08g' : ['DPA1_08g'],\n",
        "'09_01' : ['DPA1_09_01'],\n",
        "'10_01' : ['DPA1_10_01'],\n",
        "'11_01' : ['DPA1_11_01']\n",
        "}\n",
        "\n",
        "indian_rhesus['MHC_DPB_HAPLOTYPES'] = {\n",
        "'01g1' : ['DPB1_01g1'],\n",
        "'01g2' : ['DPB1_01g2'],\n",
        "'01g3' : ['DPB1_01g3'],\n",
        "'01g4' : ['DPB1_01g4'],\n",
        "'01g5' : ['DPB1_01g5'],\n",
        "'02_02' : ['DPB1_02_02'],\n",
        "'02g' : ['DPB1_02g'],\n",
        "'03g' : ['DPB1_03g'],\n",
        "'04_01' : ['DPB1_04_01'],\n",
        "'05_01' : ['DPB1_05_01'],\n",
        "'05_02' : ['DPB1_05_02'],\n",
        "'06_04' : ['DPB1_06_04'],\n",
        "'06g' : ['DPB1_06g'],\n",
        "'07g1' : ['DPB1_07g1'],\n",
        "'07g2' : ['DPB1_07g2'],\n",
        "'08_01' : ['DPB1_08_01'],\n",
        "'08_02' : ['DPB1_08_02'],\n",
        "'15_03' : ['DPB1_15_03'],\n",
        "'15g' : ['DPB1_15g'],\n",
        "'16_01' : ['DPB1_16_01'],\n",
        "'17_01' : ['DPB1_17_01'],\n",
        "'18_01' : ['DPB1_18_01'],\n",
        "'19_02' : ['DPB1_19_02'],\n",
        "'19_06' : ['DPB1_19_06'],\n",
        "'19g1' : ['DPB1_19g1'],\n",
        "'19g2' : ['DPB1_19g2'],\n",
        "'21_01' : ['DPB1_21_01'],\n",
        "'21_02' : ['DPB1_21_02'],\n",
        "'21_03' : ['DPB1_21_03'],\n",
        "'23_01' : ['DPB1_23_01'],\n",
        "'23_02' : ['DPB1_23_02'],\n",
        "'24_01' : ['DPB1_24_01']\n",
        "}\n",
        "\n",
        "### Mauritian cynomolgus macaques ###\n",
        "\n",
        "mcm = {'PREFIX' : 'Mafa'}\n",
        "\n",
        "# MCM MHC updated by Roger 29 May 2018\n",
        "\n",
        "mcm['MHC_A_HAPLOTYPES'] = {\n",
        "'M1A' : ['05_M1M2M3_A1_063g', '07_M1M2_70_156bp', '11_M1_E_02g3|E_02_nov_09,_E_02_nov_10', '04_M1_AG_05_3mis_156bp'],\n",
        "'M2A' : ['05_M1M2M3_A1_063g', '07_M1M2_70_156bp', '02_M2_G_02_06_156bp'],\n",
        "'M3A' : ['05_M1M2M3_A1_063g', '07_M3_70_156bp'],\n",
        "'M4A' : ['05_M4_A1_031_01'],\n",
        "'M5A' : ['05_M5_A1_033_01'],\n",
        "'M6A' : ['05_M6_A1_032_01', '05_M6_A1_047_01'],\n",
        "'M7A' : ['05_M7_A1_060_05']\n",
        "}\n",
        "\n",
        "mcm['MHC_B_HAPLOTYPES'] = {\n",
        "'M1B' : ['12_M1_B_134_02', '12_M1_B_152_01N'],\n",
        "'M2B' : ['12_M2_B_019_03', '12_M2_B_150_01_01'],\n",
        "'M3B' : ['12_M3_B_165_01', '12_M3_B_075_01'],\n",
        "'M4B' : ['12_M4_B_088_01', '12_M4_B_127_nov_01'],\n",
        "'M5B' : ['12_M5_B_167_01N', '12_M5_B_051_04'],\n",
        "'M6B' : ['12_M6_B17_01_g103c', '12_M6_B_095_01'],\n",
        "'M7B' : ['12_M7_B_072_02', '12_M7_B_166_01']\n",
        "}\n",
        "\n",
        "mcm['MHC_DRB_HAPLOTYPES'] = {\n",
        "'M1DR' : ['13_M1_DRB_W21_01', '13_M1_DRB_W5_01'],\n",
        "'M2DR' : ['13_M2_DRB1_10_01', '13_M2_DRB_W4_02'],\n",
        "'M3DR' : ['13_M3_DRB1_10_02', '13_M3_DRB_W49_01_01'],\n",
        "'M4DR' : ['13_M4_DRB4_01_01'],\n",
        "'M5DR' : ['13_M5_DRB4_01_02'],\n",
        "'M6DR' : ['13_M6_DRB1_04_02_01', '13_M6_DRB_W4_01'],\n",
        "'M7DR' : ['13_M7_DRB_W1_03', '13_M7_DRB_W36_05']\n",
        "}\n",
        "\n",
        "mcm['MHC_DQA_HAPLOTYPES'] = {\n",
        "'M1DQ' : ['14_M1_DQB1_18_01_01'],\n",
        "'M2DQ' : ['14_M2_DQA1_01_04'],\n",
        "'M3DQ' : ['14_M3_DQB1_16_01', '14_M3_DQA1_05_03_01'],\n",
        "'M4DQ' : ['14_M4_DQB1_06_08', '14_M4_DQA1_01_07_01'],\n",
        "'M5DQ' : ['14_M5_DQA1_01_06', '14_M5_DQB1_06_11'],\n",
        "'M6DQ' : ['14_M6_DQA1_01_08_01'],\n",
        "'M7DQ' : ['14_M7_DQA1_23_01', '14_M7_DQB1_18_14']\n",
        "}\n",
        "\n",
        "mcm['MHC_DQB_HAPLOTYPES'] = {\n",
        "'M1DQ' : ['14_M1_DQB1_18_01_01'],\n",
        "'M2DQ' : ['14_M2_DQA1_01_04'],\n",
        "'M3DQ' : ['14_M3_DQB1_16_01', '14_M3_DQA1_05_03_01'],\n",
        "'M4DQ' : ['14_M4_DQB1_06_08', '14_M4_DQA1_01_07_01'],\n",
        "'M5DQ' : ['14_M5_DQA1_01_06', '14_M5_DQB1_06_11'],\n",
        "'M6DQ' : ['14_M6_DQA1_01_08_01'],\n",
        "'M7DQ' : ['14_M7_DQA1_23_01', '14_M7_DQB1_18_14']\n",
        "}\n",
        "\n",
        "mcm['MHC_DPA_HAPLOTYPES'] = {\n",
        "'M1DP' : ['15_M1_DPA1_07_02', '15_M1_DPB1_19_03'],\n",
        "'M2DP' : ['15_M2_DPA1_07_01', '15_M2_DPB1_20_01'],\n",
        "'M3DP' : ['15_M3_DPB1_09_02'],\n",
        "'M4M7DP' : ['15_M4M7_DPB1_03_03'],\n",
        "'M5M6DP' : ['15_M5M6_DPB1_04_01']\n",
        "}\n",
        "\n",
        "mcm['MHC_DPB_HAPLOTYPES'] = {\n",
        "'M1DP' : ['15_M1_DPA1_07_02', '15_M1_DPB1_19_03'],\n",
        "'M2DP' : ['15_M2_DPA1_07_01', '15_M2_DPB1_20_01'],\n",
        "'M3DP' : ['15_M3_DPB1_09_02'],\n",
        "'M4M7DP' : ['15_M4M7_DPB1_03_03'],\n",
        "'M5M6DP' : ['15_M5M6_DPB1_04_01']\n",
        "}\n",
        "\n",
        "haplotype_dict = {'MAMU':indian_rhesus, 'MCM':mcm, 'MANE':indian_rhesus}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lq8vdlj3FHz"
      },
      "source": [
        "## Genotype miSeq data against reference FASTA\n",
        "\n",
        "This is a new implementation of the MHC genotyping pipeline. Considerations:\n",
        "\n",
        "- Integration with LabKey to reduce manual typing\n",
        "- Speed (condense identical reads pre-genotyping). This should make it more feasible to reanalyze old data with new allele databases. Current throughput is about 360 samples per hour (10 seconds per sample).\n",
        "- Flexible reporting with Pandas (create framework for adding haplotyping) and to support eventual genotyping results in LabKey\n",
        "- Export to Excel similar to current format\n",
        "- Jupyter Notebook for portability and reproducible data analysis. This is really important so we can distribute users' data and the full analysis of their results.\n",
        "\n",
        "One possibly controversial decision in this algorithm is that I selectively include identical sequences that are found  as a fraction of total reads. This runs the risk of losing some sequences that could potentially be informative. When making this decision, I thought a lot about lossless compression of music. There is a lot of discussion about whether lossy compression of music files (e.g., 320kb MP3 is distinguishable from lossless FLAC/ALAC (https://www.npr.org/sections/therecord/2015/06/02/411473508/how-well-can-you-hear-audio-quality). I think there is a parallel in MHC genotyping -- do we really need to know all MHC if they are present in very low abundance of cDNA? Could we improve genotyping by simply reporting those sequences that comprise a significant fraction of reads (set to 0.1% of total reads by default)? I would need to be convinced that this really helps.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "+ Jupyter Notebook/Jupyter Lab\n",
        "+ Python 3 (tested on anaconda distribution of Python 3.6.4)\n",
        "+ Access to dholk.primate.wisc.edu\n",
        "+ pigz (in PATH)\n",
        "+ bbmap (in PATH)\n",
        "+ bbmerge (in PATH)\n",
        "+ bbduk (in PATH)\n",
        "+ USEARCH v10 (attempts automatic installation if not available)\n",
        "+ Pandas (tested from anaconda distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GwW0uEi3FH2"
      },
      "outputs": [],
      "source": [
        "# generic functions\n",
        "\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# import logger\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "def print_status(status):\n",
        "    '''print timestamped status update'''\n",
        "    print('--[' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '] ' + status + '--')\n",
        "    log.info(status)\n",
        "\n",
        "def create_temp_folder():\n",
        "    '''make temporary folder at specified location'''\n",
        "\n",
        "    TMP_DIR = tempfile.mkdtemp(dir='/content/tmp/')\n",
        "    return TMP_DIR\n",
        "\n",
        "def close_temp_folder(tmp_dir):\n",
        "    '''destroy temporary folder after it is no longer used'''\n",
        "    os.removedirs(tmp_dir)\n",
        "\n",
        "def create_output_folder(cwd):\n",
        "    '''create timestamped output folder at specified location'''\n",
        "\n",
        "    # fetch current time\n",
        "    CURRENT_TIME = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "    # path to output folder\n",
        "    OUTPUT_FOLDER = cwd + '/' + CURRENT_TIME\n",
        "\n",
        "    # create folder if it doesn't already exist\n",
        "    if not os.path.exists(OUTPUT_FOLDER):\n",
        "        os.makedirs(OUTPUT_FOLDER)\n",
        "\n",
        "    # print output folder name\n",
        "    print_status('Output folder: ' + OUTPUT_FOLDER)\n",
        "\n",
        "    return OUTPUT_FOLDER\n",
        "\n",
        "def run_command(cmd_list, stdout_file = None, stderr_file = None):\n",
        "    '''run command with subprocess.call\n",
        "    if stdout or stderr arguments are passed, save to specified file\n",
        "    '''\n",
        "\n",
        "    import subprocess\n",
        "\n",
        "    print_status(' '.join(cmd_list)) # print status\n",
        "\n",
        "    # if neither stdout or stderr specified\n",
        "    if stdout_file is None and stderr_file is None:\n",
        "        print(cmd_list)\n",
        "        subprocess.call(cmd_list)\n",
        "\n",
        "    # if only stdout is specified\n",
        "    elif stdout_file is not None and stderr_file is None:\n",
        "        with open(stdout_file, 'w') as so:\n",
        "            subprocess.call(cmd_list, stdout = so)\n",
        "\n",
        "    # if only stderr is specified\n",
        "    elif stdout_file is None and stderr_file is not None:\n",
        "        with open(stderr_file, 'w') as se:\n",
        "            subprocess.call(cmd_list, stderr = se)\n",
        "\n",
        "    # if both stdout and stderr are specified\n",
        "    elif stdout_file is not None and stderr_file is not None:\n",
        "        with open(stdout_file, 'w') as so:\n",
        "            with open(stderr_file, 'w') as se:\n",
        "                subprocess.call(cmd_list, stdout = so, stderr = se)\n",
        "\n",
        "    else: pass\n",
        "\n",
        "def test_executable(cmd):\n",
        "    '''check that a particular command can be run as an executable'''\n",
        "\n",
        "    import shutil\n",
        "\n",
        "    assert shutil.which(cmd) is not None, 'Executable ' + cmd + ' cannot be run'\n",
        "\n",
        "def get_notebook_path(out_dir):\n",
        "    '''get name of  20835-genotyping.ipynb file in current working directory\n",
        "    copy to output folder\n",
        "    '''\n",
        "\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    cwd = os.getcwd() # get working directory\n",
        "    notebook_path = cwd + '/26887-miseq-genotyping.ipynb'\n",
        "\n",
        "    # copy to output folder\n",
        "    shutil.copy2(notebook_path, out_dir + '/' + EXPERIMENT + '.ipynb')\n",
        "\n",
        "def file_size(f):\n",
        "    '''return file size'''\n",
        "    import os\n",
        "\n",
        "    return os.stat(f).st_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLj95VtF3FH3"
      },
      "source": [
        "## Create data structure for paired-end reads\n",
        "\n",
        "Make dictionary containing R1/R2 miSeq read pairs, alternative sample identifiers, and miSeq run IDs and uses sample names as the dictionary key. This should only cause problems if multiple samples with the same name are run in the same workflow invokation, which seems unlikely. The system requires gzip-FASTQ files and will abort if FASTQ files are not gzip-compressed. This is to protect users from themselves - having uncompressed FASTQ files in the filesystem is a quick way to fill hard drives. There _is_ an uncompressed, merged FASTQ file that gets generated in the temporary intermediate files; such files are necessary for USEARCH functionality.\n",
        "\n",
        "As a convenience, animal identifiers and run information is downloaded directly from the dholk LabKey server. Data from every genotyping run should be in this system so I have built this workflow to deliberately break if the data isn't in LabKey. Just today (2018-05-25 --dho) I had to spend time scouring BaseSpace for miSeq files that weren't properly archived in our LabKey system. So it does not seem unreasonable to me to enforce correct usage of the miSeq LabKey system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAhQMF9P3FH4"
      },
      "outputs": [],
      "source": [
        "def is_gz_file(filepath):\n",
        "    '''test if file is gzip-compressed\n",
        "    return True if gzip-compressed\n",
        "    source: https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed\n",
        "    '''\n",
        "\n",
        "    import binascii\n",
        "\n",
        "    with open(filepath, 'rb') as test_f:\n",
        "        return binascii.hexlify(test_f.read(2)) == b'1f8b'\n",
        "\n",
        "def get_labkey_sample_name(R1):\n",
        "    '''lookup sample name (both GS name and client name) from miSeq ID using dholk\n",
        "    lookup miSeq run number - in Run/MetaDataId column of 20835 view\n",
        "    '''\n",
        "\n",
        "    from labkey.api_wrapper import APIWrapper\n",
        "    from labkey.exceptions import (\n",
        "        RequestError,\n",
        "        QueryNotFoundError,\n",
        "        ServerContextError,\n",
        "        ServerNotFoundError,\n",
        "    )\n",
        "    from labkey.query import Pagination, QueryFilter\n",
        "    from requests.exceptions import Timeout\n",
        "    import os\n",
        "    # dholk configuration\n",
        "    api = APIWrapper('dholk.primate.wisc.edu',\n",
        "                    'dho/Illumina',\n",
        "                    api_key='apikey|{0}'.format(api_key),\n",
        "                    use_ssl=True)\n",
        "\n",
        "    # get row matching R1 FASTQ.gz file\n",
        "    # query against basename of R1\n",
        "\n",
        "    my_results = api.query.select_rows(\n",
        "    schema_name='genotyping',\n",
        "    query_name='SequenceFiles',\n",
        "    view_name='20835',\n",
        "    filter_array=[\n",
        "        QueryFilter('DataId/Name', os.path.basename(R1), 'contains')\n",
        "    ]\n",
        "    )\n",
        "\n",
        "    # get sample ID corresponding to sample name\n",
        "    sample_id = (my_results['rows'][0])['SampleId/library_sample_name']\n",
        "    description = (my_results['rows'][0])['SampleId/description']\n",
        "    run_id = (my_results['rows'][0])['Run/MetaDataId']\n",
        "\n",
        "    # ensure sample ID exists\n",
        "    assert sample_id != '', 'Labkey sample identifier could not be identified for ' + R1_BN\n",
        "\n",
        "    return (sample_id, description, run_id)\n",
        "\n",
        "# def make_read_dict(df_sample):\n",
        "#     '''create dictionary from folder of paired-end reads\n",
        "#     expect gzip-compressed FASTQ files\n",
        "#     '''\n",
        "#     df_sample[]\n",
        "    # import glob\n",
        "\n",
        "    # # create data structure\n",
        "    # d = {}\n",
        "\n",
        "    # # find R1 files in READ_FOLDER\n",
        "    # # for name in glob.glob(read_folder + '/*R1*fastq.gz')[0:10]:  # test first three samples\n",
        "    # for name in glob.glob(read_folder + '/*R1*fastq.gz'):  # run all samples\n",
        "    #     R1 = name\n",
        "    #     R2 = R1.replace('R1', 'R2') # get R2 file path\n",
        "\n",
        "    #     # make sure paths exist - if not, throw error\n",
        "    #     assert os.path.exists(R1) == 1, 'R1 file ' + R1 + ' does not exist'\n",
        "    #     assert os.path.exists(R2) == 1, 'R2 file ' + R2 + ' does not exist'\n",
        "\n",
        "    #     # make sure files are gzip compressed\n",
        "    #     assert is_gz_file(R1) is True, 'R1 file ' + R1 + ' is not gzip compressed. Please gzip compress and try again.'\n",
        "    #     assert is_gz_file(R2) is True, 'R2 file ' + R2 + ' is not gzip compressed. Please gzip compress and try again.'\n",
        "\n",
        "    #     # get sample ID from Labkey\n",
        "    #     # get run ID from LabKey\n",
        "    #     LABKEY_IDS = get_labkey_sample_name(R1)\n",
        "    #     SAMPLE_ID = LABKEY_IDS[0]\n",
        "    #     CLIENT_ID = LABKEY_IDS[1]\n",
        "    #     RUN_ID = LABKEY_IDS[2]\n",
        "\n",
        "    #     # add FASTQ and client sample ID to dictionary\n",
        "\n",
        "\n",
        "    # description = (my_results['rows'][0])['SampleId/description']\n",
        "    # run_id = (my_results['rows'][0])['Run/MetaDataId']\n",
        "    #     d[SAMPLE_ID] = [R1, R2, CLIENT_ID, RUN_ID]\n",
        "\n",
        "    # return d\n",
        "\n",
        "def get_read_dict(df_samples_i):\n",
        "    READS={}\n",
        "    sample_list = list(df_samples_i['SampleId/library_sample_name'].unique())\n",
        "    for sample_i in sample_list:\n",
        "\n",
        "        df_samples_j  = df_samples_i[df_samples_i['SampleId/library_sample_name']== sample_i]\n",
        "\n",
        "        CLIENT_ID= list(df_samples_j['SampleId/description'])[0]\n",
        "        RUN_ID= list(df_samples_j['Run'])[0]\n",
        "        read_list = list(df_samples_j['FILEPATH'])\n",
        "\n",
        "        for filepath_i in read_list:\n",
        "            if 'R1' in filepath_i:\n",
        "                R1 = filepath_i\n",
        "                break\n",
        "        for filepath_i in read_list:\n",
        "            if 'R2' in filepath_i:\n",
        "                R2 = filepath_i\n",
        "                break\n",
        "\n",
        "        READS[sample_i] = [R1, R2, CLIENT_ID, RUN_ID]\n",
        "    return READS\n",
        "\n",
        "\n",
        "def rsync_files(source=None,\n",
        "                un=None,\n",
        "                server=None,\n",
        "                dest=None,\n",
        "                server_is_dest=True,\n",
        "                remote=True,\n",
        "                compressed=False,\n",
        "                control_path='$HOME/.ssh/%L-%r@%h:%p',\n",
        "                rsync_flag='-ahP',\n",
        "                source_from_file_list=False,\n",
        "                cwd='/',\n",
        "                checksum_only=False,\n",
        "                ignore_errors=False,\n",
        "                remove_source_files=False):\n",
        "    \"\"\"\n",
        "   this rsync_files is only for remote directory transfer only to local (or local to remote).\n",
        "   :param source: <string> source directory\n",
        "   :param un: <string> Remote username\n",
        "   :param server: <string> remote server or ip address or domain.\n",
        "   :param dest: <string> destination directory\n",
        "   :param server_is_dest: <bool> set to False if the local directory is the destination; True, if server is destination.\n",
        "   :param remote: <bool> if sending or recieving from remote site, un server is required (not None).\n",
        "   :param compressed: <bool> compress before sending to save Network but slows I/O\n",
        "   :param control_path: <string>  Allows to use the same ssh connecton (with out reopening new ones if it exists).\n",
        "   :param rsync_flag: <string> Allows to use a custom flag for file transfer (if not zipped or archive etc.)\n",
        "   :param source_from_file_list: <string> Allows you to pass a list in a (return delimited txt filepath) instead of dir\n",
        "   :param cwd: <string> change the current working directory if needed for relative paths. (source from filelist)\n",
        "   :param checksum_only: <bool> Set to true if you only want to check if the files from source/dest match by Checksum.\n",
        "   (no transfer happens if set to True, just returns false or true if the files match.)\n",
        "   :param ignore_errors: <bool> Ignore typical errors, but not all rsync versions support it so do not use.\n",
        "   :param remove_source_files: <bool> delete source files after transfer\n",
        "   :return: , Returns dictionary:\n",
        "    {'error': <bool>, if True if error while running rsync, false if not.\n",
        "                   'checksum_passed': <bool>, True if checksum passed.\n",
        "                   'checksum_failed_count': <int> of file count 0 if none failed, -1 if not running checksum only\n",
        "                   Prints to terminal: Parsed Std out to the terminal (or stderr it there was an error needed)\n",
        "   \"\"\"\n",
        "    from subprocess import Popen, PIPE\n",
        "\n",
        "    if (source is None) or (dest is None):\n",
        "        print('source or dest not declared')\n",
        "        return {'error': True,\n",
        "                'checksum_passed': False,\n",
        "                'checksum_failed_count': 1}\n",
        "\n",
        "    run_list = ['rsync']\n",
        "    use_control_path = False\n",
        "    if remote and (un is not None) and (server is not None):\n",
        "        control_path, use_control_path = check_reconnect_ssh_control_path(control_path=control_path,\n",
        "                                                                          un=un,\n",
        "                                                                          server=server)\n",
        "        if server_is_dest:\n",
        "            dest = '{0}@{1}:{2}'.format(un, server, dest)\n",
        "        else:\n",
        "            source = '{0}@{1}:{2}'.format(un, server, source)\n",
        "        # Get the rsync string and the cwd\n",
        "        if use_control_path:\n",
        "            run_list.append('-e')\n",
        "            run_list.append('ssh {0}'.format(control_path))\n",
        "    if compressed:\n",
        "        run_list.append('{0}z'.format(rsync_flag))\n",
        "    else:\n",
        "        run_list.append(rsync_flag)\n",
        "\n",
        "    if ignore_errors:\n",
        "        run_list.append('--ignore_errors')\n",
        "    if remove_source_files:\n",
        "        run_list.append('--remove-source-files')\n",
        "    if source_from_file_list:\n",
        "        if cwd != '.':\n",
        "            # files cannot have same name\n",
        "            run_list.append('--no-R')\n",
        "        run_list.append('--files-from={0}'.format(source))\n",
        "        run_list.append(cwd)\n",
        "\n",
        "    else:\n",
        "        run_list.append(source)\n",
        "\n",
        "    run_list.append(dest)\n",
        "\n",
        "    result_dict = {'error': False,\n",
        "                   'checksum_passed': True,\n",
        "                   'checksum_failed_count': 0}\n",
        "    if checksum_only:\n",
        "        run_list = ['rsync']\n",
        "        addl_list = ['-ro',\n",
        "                     '--dry-run',\n",
        "                     '--out-format=\"%f\"',\n",
        "                     '--checksum',\n",
        "                     source,\n",
        "                     dest]\n",
        "        if use_control_path:\n",
        "            run_list.append('-e')\n",
        "            run_list.append('ssh {0}'.format(control_path))\n",
        "        run_list = run_list + addl_list\n",
        "        #     p1 = subprocess.Popen(run_list, stdout=PIPE,stderr=PIPE, bufsize=1, universal_newlines=True)\n",
        "        check_error = False\n",
        "        line_count = 0\n",
        "        checksum_passed = True\n",
        "        file_count = 0\n",
        "        print(' '.join(run_list))\n",
        "        with Popen(run_list, stdout=PIPE, stderr=PIPE, bufsize=1, universal_newlines=True) as p1:\n",
        "            for line in iter(p1.stderr.readline, b''):\n",
        "                print(line)\n",
        "                if len(line) == 0:\n",
        "                    break\n",
        "                check_error = True\n",
        "                checksum_passed = False\n",
        "            for line in iter(p1.stdout.readline, b''):\n",
        "                if len(line) == 0:\n",
        "                    break\n",
        "                print('stdout', line.strip())\n",
        "                checksum_passed = False\n",
        "                file_count += 1\n",
        "        result_dict['checksum_passed'] = checksum_passed\n",
        "        result_dict['checksum_failed_count'] = file_count\n",
        "        result_dict['error'] = check_error\n",
        "\n",
        "        print('checksum_passed: {0}'.format(checksum_passed))\n",
        "        print('file_count: {0}'.format(file_count))\n",
        "        print('Error: {0}'.format(check_error))\n",
        "        return result_dict\n",
        "    else:\n",
        "        firstline = True\n",
        "        items2 = []\n",
        "        is_error = False\n",
        "        printed_first_line = False\n",
        "        print(' '.join(run_list))\n",
        "        with Popen(run_list, stdout=PIPE, stderr=PIPE, bufsize=1, universal_newlines=True) as p:\n",
        "            for line in iter(p.stdout.readline, b''):\n",
        "                if len(line) == 0:\n",
        "                    if printed_first_line:\n",
        "                        print(\" \".join(items))\n",
        "                    break\n",
        "                # print(err)\n",
        "                items = line.strip().split()\n",
        "                # Save over only if it has a % which is a status update on the transfer.\n",
        "                if len(items) > 1 and len(items2) > 1 and items2[1].endswith('%') and items[1].endswith('%'):\n",
        "                    print(\" \".join(tuple(filter(None, items2))), end='\\x1b\\r')\n",
        "                elif len(items2) > 0:\n",
        "                    print(\" \".join(items2))\n",
        "                # Print the first line if it exists else save the line to check if it is a print over update line\n",
        "                if firstline:\n",
        "                    firstline = False\n",
        "                    print(\" \".join(items))\n",
        "                else:\n",
        "                    items2 = items\n",
        "                    printed_first_line = True\n",
        "\n",
        "            # Get any error messages and final lines (final lines typically blank) by.\n",
        "            stout, sterr = p.communicate()\n",
        "            if len(stout.strip()) > 0:\n",
        "                print(stout.strip())\n",
        "            # print error if it exists and save as error\n",
        "            if len(sterr) > 0:\n",
        "                print(sterr)\n",
        "                result_dict['error'] = True\n",
        "                result_dict['checksum_passed'] = False\n",
        "                result_dict['checksum_failed_count'] = -1\n",
        "                return result_dict\n",
        "        return result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WelbzEte3FH7"
      },
      "outputs": [],
      "source": [
        "def decompress_fastq(reads):\n",
        "    '''decompress FASTQ file with pigz and return path to decompressed file'''\n",
        "\n",
        "    # ensure bbduk can be run from path\n",
        "    test_executable('pigz')\n",
        "\n",
        "    # command\n",
        "    decompress_fastq_cmd = ['pigz',\n",
        "                         '-d',\n",
        "                         '-k',\n",
        "                         reads]\n",
        "\n",
        "    # run command\n",
        "    run_command(decompress_fastq_cmd)\n",
        "\n",
        "    # return path to decompressed file\n",
        "    return os.path.splitext(reads)[0]\n",
        "def vsearch_unique(reads, out_dir):\n",
        "     # vsearch_path\n",
        "    # test_executable(vsearch_path)\n",
        "\n",
        "    READS_BN = [x for x in map(str.strip, (os.path.basename(reads)).split('.')) if x][0]\n",
        "\n",
        "    vsearch_unique_cmd = [vsearch_path,\n",
        "                          '--fastx_uniques',\n",
        "                          reads,\n",
        "                          '--sizeout',\n",
        "                          '--relabel',\n",
        "                          'Uniq',\n",
        "                          '--fastaout',\n",
        "                           out_dir + '/' + READS_BN + '.unique.fasta',]\n",
        "    # /Users/dabaker3/github/26284-genotyper/vsearch-2.21.1-macos-x86_64/bin/vsearch \\\n",
        "    # --fastx_uniques 769_26563_miseq_sample_sheet_11_5_21-R1-68281.merged.fastq.gz\n",
        "    # --relabel Uniq --sizeout --output 68281.unique.fastq.gz\n",
        "\n",
        "\n",
        "    # run command\n",
        "    run_command(vsearch_unique_cmd)\n",
        "\n",
        "    # test that output file exists before exiting function\n",
        "    assert os.path.exists(out_dir + '/' + READS_BN + '.unique.fasta') == 1, out_dir + '/' + READS_BN + '.unique.fasta' + ' does not exist'\n",
        "\n",
        "    # return unique sequence FASTQ and total number of merged reads\n",
        "    return (out_dir + '/' + READS_BN + '.unique.fasta')\n",
        "\n",
        "def usearch_unique(reads, out_dir):\n",
        "    '''expect merged gzip-compressed FASTQ files\n",
        "    run clumpify'''\n",
        "\n",
        "    import os\n",
        "\n",
        "    # ensure bbduk can be run from path\n",
        "    test_executable(USEARCH_PATH)\n",
        "\n",
        "    # make sure FASTQ files exists\n",
        "    assert os.path.exists(reads) == 1, 'FASTQ file ' + reads + ' does not exist'\n",
        "\n",
        "    # get basename for reads\n",
        "    READS_BN = [x for x in map(str.strip, (os.path.basename(reads)).split('.')) if x][0]\n",
        "\n",
        "    # decompress FASTQ for use with USEARCH\n",
        "    READS_DECOMPRESSED = decompress_fastq(reads)\n",
        "    assert os.path.exists(READS_DECOMPRESSED) == 1,  READS_DECOMPRESSED + ' does not exist'\n",
        "\n",
        "    # USEARCH unique command\n",
        "\n",
        "    usearch_unique_cmd = [USEARCH_PATH,\n",
        "                  '-fastx_uniques',\n",
        "                  READS_DECOMPRESSED,\n",
        "                  '-sizeout',\n",
        "                  '-relabel',\n",
        "                  'Uniq',\n",
        "                  '-fastaout',\n",
        "                   out_dir + '/' + READS_BN + '.unique.fasta',]\n",
        "    # /Users/dabaker3/github/26284-genotyper/vsearch-2.21.1-macos-x86_64/bin/vsearch \\\n",
        "    # --fastx_uniques 769_26563_miseq_sample_sheet_11_5_21-R1-68281.merged.fastq.gz\n",
        "    # --relabel Uniq --sizeout --output 68281.unique.fastq.gz\n",
        "\n",
        "\n",
        "    # run command\n",
        "    run_command(usearch_unique_cmd)\n",
        "\n",
        "    # test that output file exists before exiting function\n",
        "    assert os.path.exists(out_dir + '/' + READS_BN + '.unique.fasta') == 1, out_dir + '/' + READS_BN + '.unique.fasta' + ' does not exist'\n",
        "\n",
        "    # return unique sequence FASTQ and total number of merged reads\n",
        "    return (out_dir + '/' + READS_BN + '.unique.fasta')\n",
        "\n",
        "def vsearch_denoise(reads, read_ct, out_dir):\n",
        "    '''remove sequencing artifacts and chimeras by running UNOISE from the USEARCH package\n",
        "    Expects decompressed FASTA unique sequences created from fastx_uniques command\n",
        "    Preserves output sequences over a calculated minimum abundance\n",
        "    '''\n",
        "\n",
        "    import os\n",
        "\n",
        "    # ensure bbduk can be run from path\n",
        "    # test_executable(vsearch_path)\n",
        "\n",
        "    # make sure FASTA file exists\n",
        "    assert os.path.exists(reads) == 1, 'Unique FASTA file ' + reads + ' does not exist'\n",
        "\n",
        "    # get basename for reads\n",
        "    READS_BN = [x for x in map(str.strip, (os.path.basename(reads)).split('.')) if x][0]\n",
        "\n",
        "    # calculate UNOISE minuniquesize threshold\n",
        "    MIN_READS = str(calculate_threshold(min_freq=0.0002, reads=reads, read_ct=read_ct))\n",
        "    print(\"MINREADS: {0}\".format(MIN_READS))\n",
        "    # if MIN_READS < 2 (as can happen for samples with low coverage, set MIN_READS = 2)\n",
        "\n",
        "    if int(MIN_READS) < 2:\n",
        "        MIN_READS = '2'\n",
        "\n",
        "    # USEARCH unoise command\n",
        "    # include only sequences greater than min_reads threshold\n",
        "\n",
        "    vsearch_unoise_cmd = [vsearch_path,\n",
        "                  '--cluster_unoise',\n",
        "                  reads,\n",
        "                  '--minsize',\n",
        "                  MIN_READS,\n",
        "                  '--unoise_alpha',\n",
        "                  '2',\n",
        "                  '--centroids',\n",
        "                  os.path.join(out_dir, READS_BN + '.unoise.fasta' ) ]\n",
        "\n",
        "    # run command\n",
        "    run_command(vsearch_unoise_cmd)\n",
        "\n",
        "    # extract ZOTU sequences from unique FASTA\n",
        "    vsearch_chimera_cmd = [vsearch_path,\n",
        "                  '--uchime_denovo',\n",
        "                  os.path.join(out_dir, READS_BN + '.unoise.fasta' ),\n",
        "                  '--abskew',\n",
        "                  '16',\n",
        "                  '--nonchimeras',\n",
        "                  os.path.join(out_dir, READS_BN + '.zotu_descriptive.fasta' ) ]\n",
        "\n",
        "    # run command\n",
        "    run_command(vsearch_chimera_cmd, stdout_file = os.path.join(out_dir,'stdout.txt'), stderr_file = os.path.join(out_dir,'stderr.txt'))\n",
        "\n",
        "    # test that output file exists before exiting function\n",
        "    assert os.path.exists( os.path.join(out_dir, READS_BN + '.zotu_descriptive.fasta' )) == 1, out_dir + '/' + READS_BN + '.zotu_descriptive.fasta' + ' does not exist'\n",
        "\n",
        "    # return unique sequence FASTQ and total number of merged reads\n",
        "    return  os.path.join(out_dir, READS_BN + '.zotu_descriptive.fasta' )\n",
        "\n",
        "def parse_unoise_output(denoise_stats, zotu_tmp_fasta, reads_bn, out_dir):\n",
        "    '''UNOISE3 creates a new FASTA file without size annotations\n",
        "    Parse zotu stats file to add size annotations to zotus so read count is preserved\n",
        "    '''\n",
        "\n",
        "    import re\n",
        "\n",
        "    # make list of Zotu identifiers\n",
        "\n",
        "    descriptive_names = []\n",
        "\n",
        "    # read unoise_tabbed_output\n",
        "    with open(denoise_stats) as fp:\n",
        "        for line in fp:\n",
        "            if 'zotu' in line: # save only lines that correspond to zotus\n",
        "                descriptive_names.append(line.split(\"\\t\")[0]) # save descriptive name, such as Uniq413;size=9;\n",
        "\n",
        "    # read zotu_tmp_fasta\n",
        "    # replace non-informative Zotu name with descriptive name\n",
        "    ct = 0 # initialize counter\n",
        "    with open(out_dir + '/' + reads_bn + '.zotu_descriptive.fasta', 'w') as w: # save zotus to new file\n",
        "        with open(zotu_tmp_fasta) as fp:\n",
        "            for line in fp:\n",
        "                if '>' in line: # save only lines that correspond to zotus\n",
        "                    w.write(re.sub('>Zotu[0-9]*', '>' + descriptive_names[ct], line)) # write descriptive fasta header\n",
        "                    ct = ct + 1\n",
        "                else:\n",
        "                    w.write(line) # write sequence lines as-is\n",
        "\n",
        "    return out_dir + '/' + reads_bn + '.zotu_descriptive.fasta'\n",
        "\n",
        "def count_reads(reads):\n",
        "    '''count number of reads in a FASTQ/FASTA file\n",
        "    use bbmap stats.sh\n",
        "    return count as integer\n",
        "    '''\n",
        "\n",
        "    import subprocess\n",
        "\n",
        "    # path to stats.sh\n",
        "\n",
        "\n",
        "    # ensure stats.sh can be run from path\n",
        "    test_executable(STATS_PATH)\n",
        "\n",
        "    # make sure FASTQ input file exists\n",
        "    assert os.path.exists(reads) == 1, 'Read file ' + reads + ' does not exist'\n",
        "\n",
        "    # stats command\n",
        "    stats_cmd = [STATS_PATH,\n",
        "                  'in=' + reads,\n",
        "                  'format=4']\n",
        "\n",
        "    # run command\n",
        "    output = subprocess.check_output(stats_cmd).decode(\"utf-8\")\n",
        "\n",
        "    # filter output\n",
        "    # get first entry on second line, which will always be sequence length\n",
        "    read_ct = (output.split('\\n')[1]).split('\\t')[0]\n",
        "\n",
        "    return int(read_ct)\n",
        "\n",
        "def calculate_threshold(min_freq, reads, read_ct):\n",
        "    '''determine the -minuniquesize parameter for usearch unique\n",
        "    count number of reads in FASTQ file and then return threshold\n",
        "    0.001 would save unique sequences greater than 0.1% of all sequences in dataset'''\n",
        "\n",
        "    # multiply read threshold by read count\n",
        "    minuniquesize = int(min_freq * read_ct)\n",
        "\n",
        "    # return threshold and number of total reads)\n",
        "    return (minuniquesize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdM7_7d3FH5"
      },
      "source": [
        "## Remove primers with bbduk\n",
        "\n",
        "Need to remove primer sequences from left and right ends of individual reads before merging. It is essential to do this in multiple steps (left-end trimming first; right end trimming second) to prevent removal of primers from incorrect locations within the sequences. The default k-mer size for bbduk is 50 so if this is not set to a number as small or smaller than the smallest PCR primer trimming will not work correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCMkgkNr3FH5"
      },
      "outputs": [],
      "source": [
        "def remove_primers(R1, R2, primers, out_dir):\n",
        "    '''expect gzip-compressed R1 and R2 FASTQ files\n",
        "    run bbduk to remove primers\n",
        "    first remove from left end of sequence\n",
        "    then remove from right end of sequence\n",
        "    save results to temporary directory'''\n",
        "\n",
        "    import os\n",
        "\n",
        "    # path to bbduk\n",
        "\n",
        "    # ensure bbduk can be run from path\n",
        "    test_executable(BBDUK_PATH)\n",
        "\n",
        "    # make sure primer file exists\n",
        "    assert os.path.exists(primers) == 1, 'Specified primer file ' + primers + ' does not exist'\n",
        "\n",
        "    # get basename for R1 and R2\n",
        "    R1_BN = os.path.splitext(os.path.basename(R1))[0]\n",
        "    R2_BN = os.path.splitext(os.path.basename(R2))[0]\n",
        "\n",
        "    # trim from left\n",
        "    left_primer_cmd = [BBDUK_PATH,\n",
        "                      'in=' + R1,\n",
        "                      'in2='+ R2,\n",
        "                      'ref=' + \"'\" + primers+  \"'\",\n",
        "                      'ktrim=l',\n",
        "                      'k=15',\n",
        "                      'restrictleft=30',\n",
        "                      'out=' + out_dir + '/' + R1_BN + '_l.fastq.gz',\n",
        "                      'out2=' + out_dir + '/' + R2_BN + '_l.fastq.gz']\n",
        "\n",
        "    # next trim from right\n",
        "    right_primer_cmd = [BBDUK_PATH,\n",
        "                      'in=' + out_dir + '/' + R1_BN + '_l.fastq.gz',\n",
        "                      'in2='+ out_dir + '/' + R2_BN + '_l.fastq.gz',\n",
        "                      'ref=' + \"'\" + primers + \"'\",\n",
        "                      'ktrim=r',\n",
        "                      'restrictright=30',\n",
        "                      'k=15',\n",
        "                      'out=' + out_dir + '/' + R1_BN + '_lr.fastq.gz',\n",
        "                      'out2=' + out_dir + '/' + R2_BN + '_lr.fastq.gz']\n",
        "\n",
        "    # run left trim commands\n",
        "    run_command(left_primer_cmd)\n",
        "\n",
        "    # make sure output files from left primer trim exist\n",
        "    # assert os.path.exists(out_dir + '/' + R1_BN + '_l.fastq.gz') == 1, out_dir + '/' + R1_BN + '_l.fastq.gz' + ' does not exist'\n",
        "    # assert os.path.exists(out_dir + '/' + R2_BN + '_l.fastq.gz') == 1, out_dir + '/' + R2_BN + '_l.fastq.gz' + ' does not exist'\n",
        "\n",
        "    # run right trim command\n",
        "    run_command(right_primer_cmd)\n",
        "\n",
        "    # make sure output files from right primer trim exist\n",
        "    # assert os.path.exists(out_dir + '/' + R1_BN + '_lr.fastq.gz') == 1, out_dir + '/' + R1_BN + '_lr.fastq.gz' + ' does not exist'\n",
        "    # assert os.path.exists(out_dir + '/' + R2_BN + '_lr.fastq.gz') == 1, out_dir + '/' + R2_BN + '_lr.fastq.gz' + ' does not exist'\n",
        "\n",
        "    # return output files as tuple\n",
        "    return (out_dir + '/' + R1_BN + '_lr.fastq.gz', out_dir + '/' + R2_BN + '_lr.fastq.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfDiEEAy3FH6"
      },
      "source": [
        "## Merge reads\n",
        "\n",
        "After trimming, merge overlapping reads and use these merged reads for miSeq genotyping. bbmerge defaults are inconsistent when merging reads from longer amplicons (such as the 280bp MHC class II amplicons) that do not have a long overlap. Performance can be improved by first merging with the default parameters but then repeating merging after applying 3' trims of increasing stringency. Roger and I empirically tested different trim qualities up to 40 and discovered that performance does not improve beyond a trip of 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOkZW-js3FH6"
      },
      "outputs": [],
      "source": [
        "def merge_reads(R1, R2, out_dir):\n",
        "    '''expect gzip-compressed R1 and R2 FASTQ files\n",
        "    run bbmerge'''\n",
        "\n",
        "    import os\n",
        "\n",
        "    # path to bbduk\n",
        "\n",
        "\n",
        "    # ensure bbduk can be run from path\n",
        "    test_executable(BBMERGE_PATH)\n",
        "\n",
        "    # make sure R1 and R2 files exists\n",
        "    assert os.path.exists(R1) == 1, 'R1 file ' + R1 + ' does not exist'\n",
        "    assert os.path.exists(R2) == 1, 'R1 file ' + R2 + ' does not exist'\n",
        "\n",
        "    # get basename for R1\n",
        "    R1_BN = [x for x in map(str.strip, (os.path.basename(R1)).split('.')) if x][0]\n",
        "\n",
        "    # merge read command\n",
        "    merge_cmd = [BBMERGE_PATH,\n",
        "                  'in=' + R1,\n",
        "                  'in2='+ R2,\n",
        "                  'qtrim2=r',\n",
        "                  'trimq=10,15,20,25,30',\n",
        "                  'pfilter=0.1',\n",
        "                  'out=' + out_dir + '/' + R1_BN + '.merged.fastq.gz']\n",
        "\n",
        "    # run command\n",
        "    run_command(merge_cmd)\n",
        "\n",
        "    # test that output file exists before exiting function\n",
        "    assert os.path.exists(out_dir + '/' + R1_BN + '.merged.fastq.gz') == 1, out_dir + '/' + R1_BN + '.merged.fastq.gz' + ' does not exist'\n",
        "\n",
        "    # return output merged FASTQ\n",
        "    return out_dir + '/' + R1_BN + '.merged.fastq.gz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS2fKCTJ3FH7"
      },
      "source": [
        "## Find unique sequences and remove chimeras using USEARCH\n",
        "A key performance enhancement in this workflow is recognizing that it is easier to map an amplicon sequence that occurs 5,000 times once, after adding a header indicating that there are 5,000 identical copies of the sequence, than mapping each of the 5,000 sequences individually. USEARCH fastx_uniques condenses a set of FASTQ sequences into its unique members. This tool requires decompressed FASTQ files, so the merged FASTQ files are decompressed prior to running USEARCH. To maximize performance, decompression uses the multithreaded pigz tool that should be installed in the user's path.\n",
        "\n",
        "Another USEARCH tool, UNOISE3, purports to find \"authentic\" sequences by removing sequencing artifacts and chimeric sequences. Empirically, this tool performs quite well and using it to remove false positives contributes to improved genotyping accuracy and haplotying imputing. It is important to remove incomplete-length amplicon sequences before running UNOISE3 -- in the workflow, unique reads are first mapped to the reference sequence and any that are partial length are removed prior to running UNOISE3.\n",
        "\n",
        "As described in the introduction, another innovation in this workflow is not reporting poorly supported genotypes below a specified threshold. This is calculated by determining the total number of merged reads in each sample and calculating a minimum read_abundance to report as UNOISE3 output. I initially set this to 0.1%, but Roger thought 0.02% of total reads is a more appropriate threshold. This could be adjusted in the future if there is a consensus this value is too high or too low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-17Mun263FH8"
      },
      "source": [
        "## Map unique reads to reference\n",
        "\n",
        "bbmap in semiperfect mode is used for read mapping. According to the documentation:\n",
        "\n",
        "```\n",
        "semiperfectmode=f       Allow only perfect and semiperfect (perfect except for\n",
        "                        N's in the reference) mappings.\n",
        "```\n",
        "\n",
        "This function is used for two different purposes. First, it is used to output FASTA files following mapping unique reads and before denoising to remove incomplete length amplicons. Second, it is used to output SAM files after mapping unique, denoised sequences. The SAM file is what is parsed to determine a sample's genotype. In both cases, ambiguous mappings are discarded since there should only be one reference sequence to which each read maps (or else the reference sequences themselves are ambiguous, which shouldn't happen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDRdqPb13FH9"
      },
      "outputs": [],
      "source": [
        "def map_semiperfect(reads, ref, out_dir, out_fmt):\n",
        "    '''expect non-compressed FASTQ file of unique sequences\n",
        "    run bbmap in semiperfect mode to map reads\n",
        "    return mapped reads in specified format (out_fmt)\n",
        "    this is needed because mapped reads are returned as .fasta when removing partial-length mathc\n",
        "    and .sam when reporting allele calls\n",
        "    Also need to set ordered=t to keep reads in size-descending order for UNOISE3 to work correctly\n",
        "    '''\n",
        "\n",
        "    import os\n",
        "\n",
        "    # path to bbmap\n",
        "\n",
        "\n",
        "    # ensure bbmap can be run from path\n",
        "    test_executable(BBMAP_PATH)\n",
        "\n",
        "    # make sure FASTQ input file and reference file exists\n",
        "    assert os.path.exists(reads) == 1, 'Unique FASTA file ' + reads + ' does not exist'\n",
        "    assert os.path.exists(ref) == 1, 'Reference FASTA file ' + ref + ' does not exist'\n",
        "\n",
        "    # get basename for reads\n",
        "    READS_BN = [x for x in map(str.strip, (os.path.basename(reads)).split('.')) if x][0]\n",
        "\n",
        "    # bbmap command\n",
        "    # do not make SAM header because this interferes with pandas parsing\n",
        "    # toss all ambiguously mapped reads since the reference database should be unambiguous\n",
        "    if isinstance(BBMAP2_PATH,list):\n",
        "        bbmap_cmd = BBMAP2_PATH + ['in=' + reads,\n",
        "                  'outm=' + out_dir + '/' + READS_BN + '.' + out_fmt,\n",
        "                  'noheader=t',\n",
        "                  'nodisk=t',\n",
        "                  'ambiguous=toss',\n",
        "                  'ref='  + ref,\n",
        "                  'ordered=t',\n",
        "                  'semiperfectmode=t']\n",
        "\n",
        "    else:\n",
        "        bbmap_cmd = [BBMAP_PATH,\n",
        "                    'in=' + reads,\n",
        "                    'outm=' + out_dir + '/' + READS_BN + '.' + out_fmt,\n",
        "                    'noheader=t',\n",
        "                    'nodisk=t',\n",
        "                    'ambiguous=toss',\n",
        "                    'ref=' + \"'\" + ref + \"'\",\n",
        "                    'ordered=t',\n",
        "                    'semiperfectmode=t']\n",
        "\n",
        "    # run command\n",
        "    run_command(bbmap_cmd)\n",
        "\n",
        "    # test that output file exists before exiting function\n",
        "    assert os.path.exists(out_dir + '/' + READS_BN + '.' + out_fmt) == 1, out_dir + '/' + READS_BN + '.' + out_fmt + ' does not exist'\n",
        "\n",
        "    # return BAM file of mapped reads\n",
        "    return out_dir + '/' + READS_BN + '.' + out_fmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpJ9vuIR3FH9"
      },
      "source": [
        "## Parse SAM file\n",
        "\n",
        "Use Pandas to parse the SAM output from bbmap. This is necessary to extract the columns necessary for genotyping and to aggregate read counts from multiple unique FASTQ sequences that map uniquely to a single reference target. This can happen when there are different soft clips outside of the reference sequence.\n",
        "\n",
        "The genotypes are used to determine which haplotypes are present in a sample. Each locus is considered individually against a dictionary of haplotype/genotype definitions. It is designed to be conservative -- if the algorithm cannot confidently assign a haplotype, it will report an error instead of guessing, possibly incorrectly. It is better to have manual intervention than incorrect automatic haplotype assignments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E86Jpyw3FH9"
      },
      "outputs": [],
      "source": [
        "def parse_sam(mapped_sam, sample_id, total_read_ct, client_id, experiment, run_id, species):\n",
        "    '''parse SAM output file to determine number of reads per reference\n",
        "    required because bbmap nor usearch respect read abundance when mapping\n",
        "    '''\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    # read SAM file\n",
        "    df = pd.read_csv(mapped_sam, sep='\\t', header=None)\n",
        "\n",
        "    # extract read count and allele columns\n",
        "    genotyping_df = df[[0, 2]]\n",
        "\n",
        "    # rename columns\n",
        "    genotyping_df = genotyping_df.rename(columns={0: 'read_ct', 2: 'allele'})\n",
        "\n",
        "    # extract size value from first column\n",
        "\n",
        "    genotyping_df['read_ct'] = genotyping_df['read_ct'].str.replace('Uniq[0-9]*;size=', '',regex=True) # Uniq1size=6350\n",
        "    genotyping_df['read_ct'] = genotyping_df['read_ct'].str.replace(';', '')\n",
        "\n",
        "    # convert count column to numeric\n",
        "    genotyping_df[['read_ct']] = genotyping_df[['read_ct']].apply(pd.to_numeric)\n",
        "\n",
        "    # get total number of mapped reads\n",
        "    mapped_reads = genotyping_df['read_ct'].sum()\n",
        "\n",
        "    # group by allele and aggregate read_ct for identical alleles\n",
        "    # likely due to differently trimmed unique sequences mapping to the same reference\n",
        "    genotyping_df = genotyping_df.groupby(['allele']).agg({'read_ct':'sum'})\n",
        "\n",
        "    # reset index\n",
        "    genotyping_df = genotyping_df.reset_index()\n",
        "\n",
        "    # add sample_id as column\n",
        "    genotyping_df['gs_id'] = sample_id\n",
        "\n",
        "    # add mapped_read_ct as column\n",
        "    genotyping_df['mapped_read_count'] = mapped_reads\n",
        "\n",
        "    # add total_read_ct as column\n",
        "    genotyping_df['total_read_count'] = total_read_ct\n",
        "\n",
        "    # add percent_unmapped as column\n",
        "    genotyping_df['percent_reads_unmapped'] = round(100 - (mapped_reads / total_read_ct * 100), 1)\n",
        "\n",
        "    # add client_id as column\n",
        "    genotyping_df['client_id'] = client_id\n",
        "\n",
        "    # add experiment as column\n",
        "    genotyping_df['experiment'] = experiment\n",
        "\n",
        "    # add run_id as column\n",
        "    genotyping_df['run_id'] = run_id\n",
        "\n",
        "    # evaluate haplotypes\n",
        "    # MHC-A\n",
        "    MHC_A = call_haplotypes(locus=str(species['PREFIX'] + '-A'), locus_haplotype_definitions=species['MHC_A_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-A Haplotype 1'] = MHC_A[0]\n",
        "    genotyping_df['MHC-A Haplotype 2'] = MHC_A[1]\n",
        "\n",
        "    # MHC-B\n",
        "    MHC_B = call_haplotypes(locus=str(species['PREFIX'] + '-B'), locus_haplotype_definitions=species['MHC_B_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-B Haplotype 1'] = MHC_B[0]\n",
        "    genotyping_df['MHC-B Haplotype 2'] = MHC_B[1]\n",
        "\n",
        "    # MHC-DRB\n",
        "    MHC_DRB = call_haplotypes(locus=str(species['PREFIX'] + '-DRB'), locus_haplotype_definitions=species['MHC_DRB_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-DRB Haplotype 1'] = MHC_DRB[0]\n",
        "    genotyping_df['MHC-DRB Haplotype 2'] = MHC_DRB[1]\n",
        "\n",
        "    # MHC-DQA\n",
        "    MHC_DQA = call_haplotypes(locus=str(species['PREFIX'] + '-DQA'), locus_haplotype_definitions=species['MHC_DQA_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-DQA Haplotype 1'] = MHC_DQA[0]\n",
        "    genotyping_df['MHC-DQA Haplotype 2'] = MHC_DQA[1]\n",
        "\n",
        "    # MHC-DQB\n",
        "    MHC_DQB = call_haplotypes(locus=str(species['PREFIX'] + '-DQB'), locus_haplotype_definitions=species['MHC_DQB_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-DQB Haplotype 1'] = MHC_DQB[0]\n",
        "    genotyping_df['MHC-DQB Haplotype 2'] = MHC_DQB[1]\n",
        "\n",
        "    # MHC-DPA\n",
        "    MHC_DPA = call_haplotypes(locus=str(species['PREFIX'] + '-DPA'), locus_haplotype_definitions=species['MHC_DPA_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-DPA Haplotype 1'] = MHC_DPA[0]\n",
        "    genotyping_df['MHC-DPA Haplotype 2'] = MHC_DPA[1]\n",
        "\n",
        "    # MHC-DPB\n",
        "    MHC_DPB = call_haplotypes(locus=str(species['PREFIX'] + '-DPB'), locus_haplotype_definitions=species['MHC_DPB_HAPLOTYPES'], df=genotyping_df)\n",
        "    genotyping_df['MHC-DPB Haplotype 1'] = MHC_DPB[0]\n",
        "    genotyping_df['MHC-DPB Haplotype 2'] = MHC_DPB[1]\n",
        "\n",
        "    # add comments field\n",
        "    genotyping_df['Comments'] = ''\n",
        "\n",
        "    return genotyping_df\n",
        "\n",
        "def call_haplotypes(locus, locus_haplotype_definitions, df):\n",
        "    '''specify locus (e.g., Mamu-A) to haplotype and provide dictionary of haplotype definitions\n",
        "    update specified pandas dataframe\n",
        "    '''\n",
        "    # convert alleles in dataframe to string\n",
        "    # makes it possible to search for values more easily\n",
        "    allele_str = df['allele'].to_string(header=False, index=False)\n",
        "\n",
        "    # create list to store haplotypes for a sample\n",
        "    sample_haplotypes = []\n",
        "\n",
        "    # loop through haplotypes\n",
        "    for haplotype, alleles in locus_haplotype_definitions.items():\n",
        "\n",
        "        # if all diagnostic alleles for a haplotype are present\n",
        "        # save haplotype name to list\n",
        "        if all((x) in allele_str for x in alleles):\n",
        "            sample_haplotypes.append(haplotype)\n",
        "\n",
        "    # evaluate haplotypes\n",
        "        if len(sample_haplotypes) == 0: # if there are no haplotypes, that isn't possible\n",
        "            # special exception for MCM A1*063 which is often undercalled but is very important\n",
        "            if locus == 'Mafa-A' and '05_M1M2M3_A1_063g' in allele_str:\n",
        "                h1 = 'A1_063'\n",
        "                h2 = '-'\n",
        "            else:\n",
        "                h1 = 'ERR: NO HAP'\n",
        "                h2 = 'ERR: NO HAP'\n",
        "        elif len(sample_haplotypes) == 1:\n",
        "            # special exception for MCM A1*063 which is often undercalled but is very important\n",
        "            # if A1_063 is present in genotypes but M1, M2, and M3 are not present in single called haplotype, add A1_063 to haplotype2\n",
        "            if locus == 'Mafa-A'and '05_M1M2M3_A1_063g' in allele_str and not any(y in sample_haplotypes[0] for y in ('M1A', 'M2A', 'M3A')):\n",
        "                h1 = sample_haplotypes[0]\n",
        "                h2 = 'A1_063'\n",
        "            else:\n",
        "                h1 = sample_haplotypes[0]\n",
        "                h2 = '-'\n",
        "        elif len(sample_haplotypes) == 2:\n",
        "            h1 = sample_haplotypes[0]\n",
        "            h2 = sample_haplotypes[1]\n",
        "        elif len(sample_haplotypes) > 2:\n",
        "            h1 = 'ERR: TMH (' + ', '.join(sample_haplotypes) + ')'\n",
        "            h2 = 'ERR: TMH (' + ', '.join(sample_haplotypes) + ')'\n",
        "\n",
        "    # DPA, DPB, DQA, DQB can only have two genotypes though other loci can have more\n",
        "    # for these loci, error if more than two genotypes are reported\n",
        "\n",
        "    # test number of rows that match locus, if > 2 set h1 and h2 to error\n",
        "    # need to explicitly cast allele as string\n",
        "    diploid_loci = ['DPA', 'DPB', 'DQA', 'DQB']\n",
        "\n",
        "    for i in diploid_loci:\n",
        "        if (i in locus):\n",
        "            if df.allele.astype(str).str.contains(i).sum() > 2:\n",
        "                h1 = 'ERR: TMG'\n",
        "                h2 = 'ERR: TMG'\n",
        "\n",
        "    return (h1, h2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HKlI9yj3FH-"
      },
      "source": [
        "## Data reporting\n",
        "\n",
        "A major challenge of the current system is how it takes to generate a finalized report from genotyping data. By leveraging integration with LabKey and Pandas, hopefully this can be improved. The cell below specifies haplotype defintiions in an editable form. Because this entire file is included in the output folder, it provides an unambiguous way of showing which definition set is used for which analysis.\n",
        "\n",
        "The second cell takes a columnal list of genotypes and converts it to a PivotTable int he format that genotypes and haplotypes are typically reported to Genetics Services clients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_mYptDh3FH-"
      },
      "outputs": [],
      "source": [
        "def pivot_pandas(df):\n",
        "    '''Create pivot table from genotyping Pandas dataframe'''\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    # add locus identifier column to genotypes\n",
        "\n",
        "    df['locus'] = df['allele'].str.replace('_.*', '', regex=True)\n",
        "\n",
        "    # pivot data\n",
        "    DF_PIVOT = pd.pivot_table(df,\n",
        "                                         index=['locus','allele'],\n",
        "                                         columns=[\n",
        "                                                  'gs_id',\n",
        "                                                  'client_id',\n",
        "                                                  'mapped_read_count',\n",
        "                                                  'total_read_count',\n",
        "                                                  'percent_reads_unmapped',\n",
        "                                                 'MHC-A Haplotype 1',\n",
        "                                                'MHC-A Haplotype 2',\n",
        "                                                'MHC-B Haplotype 1',\n",
        "                                                'MHC-B Haplotype 2',\n",
        "                                                'MHC-DRB Haplotype 1',\n",
        "                                                'MHC-DRB Haplotype 2',\n",
        "                                                'MHC-DQA Haplotype 1',\n",
        "                                                'MHC-DQA Haplotype 2',\n",
        "                                                'MHC-DQB Haplotype 1',\n",
        "                                                'MHC-DQB Haplotype 2',\n",
        "                                                'MHC-DPA Haplotype 1',\n",
        "                                                'MHC-DPA Haplotype 2',\n",
        "                                                'MHC-DPB Haplotype 1',\n",
        "                                                'MHC-DPB Haplotype 2',\n",
        "                                                 'Comments',\n",
        "                                                 'experiment',\n",
        "                                                  'run_id'],\n",
        "                                         values='read_ct',\n",
        "                                         aggfunc=sum)\n",
        "\n",
        "\n",
        "    # to manipulate the pandas dataframe with multiindexes a lot of steps are involved\n",
        "    # we want to:\n",
        "    # break apart the database allele name into the allele group name and ambiguous individual alleles\n",
        "    # add a column showing how many times each allele occurs in a dataset\n",
        "    # add a column showing how many reads support each allele call in a dataset\n",
        "    # reorder the columns to match the desired Excel display\n",
        "\n",
        "    # create list of sample names\n",
        "    # this will be needed when reordering columns\n",
        "\n",
        "    SAMPLE_COLUMNS = [] # list to store data columns\n",
        "    for count, i in enumerate(DF_PIVOT.columns): # iterate over data columns\n",
        "        SAMPLE_COLUMNS.append(DF_PIVOT.columns[count][0]) # add sample name to list\n",
        "\n",
        "\n",
        "    # extract database allele names to split into allele groups and groups of ambiguous sequences comprising these groups\n",
        "    t = DF_PIVOT.index.values # get all values in dataframe, yields list where allele is in position [1]\n",
        "    l = [] # initalize list to store allele\n",
        "    for i in t: # iterate over values\n",
        "        l.append(i[1]) # save allele name to list\n",
        "\n",
        "    # create column with allele groupings\n",
        "    DF_PIVOT['allele_group'] = l # create new column containing full allele sequences from database\n",
        "    DF_PIVOT['allele_group'] = DF_PIVOT['allele_group'].astype(str).str.replace('\\|.*', '', regex=True) # remove ambiguous allele names to yield Mamu_A1_004g\n",
        "\n",
        "    # create column with ambiguous alleles for each grouping\n",
        "    DF_PIVOT['ambiguous_alleles'] = l # create new column containing ambiguous alleles\n",
        "    DF_PIVOT['ambiguous_alleles'] = DF_PIVOT['ambiguous_alleles'].astype(str).str.replace('.*\\|', '', regex=True) # remove allele grouping information to yield A1_004_01_01,_A1_004_01_02,_A1_004_02_01,_A1_004_02_02,_A1_004_05,_A1_004_06\n",
        "\n",
        "    # get number of data columns\n",
        "    TOTAL_COLUMNS = (len(DF_PIVOT.columns)) # count number of data columns\n",
        "\n",
        "    # select columns containing genotype data\n",
        "    # operate specifically on these columns in the next steps\n",
        "    DATA_COLUMNS = DF_PIVOT.iloc[:,0:TOTAL_COLUMNS]\n",
        "\n",
        "    # get number of NaN values per genotype\n",
        "    NULL_COLUMNS = DATA_COLUMNS.isnull().sum(axis=1).tolist() # count number of NaN columns\n",
        "\n",
        "    # subtract number of null columns from the number of data columns to get number of columns with genotype\n",
        "    DF_PIVOT['obs_count'] = NULL_COLUMNS\n",
        "    DF_PIVOT['obs_count'] = TOTAL_COLUMNS - DF_PIVOT['obs_count']\n",
        "    # pandas used to treat columns as INT by default with nan now it is all objects\n",
        "    # so errors are being thrown and numeric=True will just give \"0\"\n",
        "    col_list = []\n",
        "    for col_i in DATA_COLUMNS.columns:\n",
        "        try:\n",
        "            DATA_COLUMNS[col_i] = pd.to_numeric(DATA_COLUMNS[col_i])\n",
        "            col_list.append(col_i)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # sum number of reads per genotype\n",
        "    DF_PIVOT['sum_genotype_read_ct'] = DATA_COLUMNS[col_list].sum(axis=1).tolist()\n",
        "\n",
        "    # flatten dataframe\n",
        "    # this removes the allele index\n",
        "    DF_PIVOT = DF_PIVOT.reset_index(drop=True)\n",
        "\n",
        "    # create list with columns as desired in output Excel file\n",
        "    REORDERED_COLUMNS = ['allele_group',\n",
        "                         'sum_genotype_read_ct',\n",
        "                         'obs_count',\n",
        "                         ]\n",
        "\n",
        "    for i in SAMPLE_COLUMNS: # iterate over sample columns\n",
        "        REORDERED_COLUMNS.append(i) # add samples to ordered columns\n",
        "\n",
        "    REORDERED_COLUMNS.append('ambiguous_alleles')\n",
        "\n",
        "    DF_PIVOT = DF_PIVOT[REORDERED_COLUMNS] # change column order\n",
        "\n",
        "    return DF_PIVOT\n",
        "\n",
        "# def generate_excel_report(df, out_dir):\n",
        "#     '''make report format in Excel comparable to what GS currently uses\n",
        "#     though I'd rather see the pandas dataframe loaded into LabKey directly,\n",
        "#     generating an Excel report directly is the best way to troubleshoot this program. After people\n",
        "#     accept the Excel report, we can load the Pandas data into LabKey and then run the Excel reporting\n",
        "#     function to generate the same report from LabKey data.\n",
        "#     '''\n",
        "\n",
        "def generate_excel_report(df, out_dir):\n",
        "    '''make report format in Excel comparable to what GS currently uses\n",
        "    though I'd rather see the pandas dataframe loaded into LabKey directly,\n",
        "    generating an Excel report directly is the best way to troubleshoot this program. After people\n",
        "    accept the Excel report, we can load the Pandas data into LabKey and then run the Excel reporting\n",
        "    function to generate the same report from LabKey data.\n",
        "\n",
        "    Try to decompose pandas dataframe to a series of lists for more flexible control over Excel formatting\n",
        "    '''\n",
        "\n",
        "    import xlsxwriter\n",
        "    import numpy as np\n",
        "\n",
        "    ## Configure Excel Report ##\n",
        "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
        "    workbook  = xlsxwriter.Workbook(out_dir + '/pivot.xlsx')\n",
        "    worksheet = workbook.add_worksheet()\n",
        "\n",
        "    # freeze top 20 rows and left three columns\n",
        "    worksheet.freeze_panes(22, 3)\n",
        "\n",
        "    # add decimal formatting to columns with signals\n",
        "    worksheet.set_column('A:A', 50)\n",
        "\n",
        "    # get dataframe as list\n",
        "    df_header = df.columns.tolist()\n",
        "\n",
        "    ## create Excel formats ##\n",
        "    # add header_id format that will be applied to top rows of table (rows 1-22)\n",
        "    header_format = workbook.add_format({\n",
        "        'bold': True,\n",
        "        'align': 'left',\n",
        "        'border': 0})\n",
        "\n",
        "    # format for header number fields (e.g. mapped read_count)\n",
        "    # uses thousands separator to make these fields easier to read\n",
        "    header_number_format = workbook.add_format({\n",
        "        'bold': True,\n",
        "        'align': 'left',\n",
        "        'border': 0,\n",
        "        'num_format': '#,###'})\n",
        "\n",
        "    if 'Mafa' in SPECIES.values():\n",
        "        # conditional formatting for MCM haplotype coloring\n",
        "        # this is pretty painful to read - sorry for this\n",
        "        # the key of mcm_haplotype_formats is the haplotype identifier - it is also used as a search term for conditional formatting\n",
        "        # the value of mcm_haplotype_formats contains a list of background colors [0] and font colors [1] for haplotype highlighting\n",
        "        # these background values are also used to highlight alleles:\n",
        "        # M5 has a yellow background color which is almost impossible to read as a font color, so a secondary font color is used in this case\n",
        "\n",
        "        mcm_haplotype_formats = {'M1' : ['#0c0000', '#ffffff'],\n",
        "                                 'M2' : ['#FF0000', '#ffffff'],\n",
        "                                 'M3' : ['#0000FF', '#ffffff'],\n",
        "                                 'M4' : ['#008000', '#ffffff'],\n",
        "                                 'M5' : ['#FFFF00', '#0c0000', '#b2b200'],\n",
        "                                 'M6' : ['#808080', '#ffffff'],\n",
        "                                 'M7' : ['#800080', '#ffffff']}\n",
        "\n",
        "        # create conditional formats for the 'header' portion of the Excel file that contains per-sample summary statistics and genotype information\n",
        "        for key, value in mcm_haplotype_formats.items():\n",
        "\n",
        "            haplotype_highlight_format = workbook.add_format({'bg_color': value[0], 'font_color': value[1]})\n",
        "\n",
        "            worksheet.conditional_format('D6:ZZ19', {'type': 'text',\n",
        "                                             'criteria': 'containing',\n",
        "                                             'value': key,\n",
        "                                             'format': haplotype_highlight_format})\n",
        "\n",
        "        # create conditional formats for the 'allele' column of the Excel file (the first column)\n",
        "        # two types of conditional formats:\n",
        "        # 1. Color cells containing diagnostic genotypes\n",
        "        # This requires a separate conditional format for every diagnostic genotype\n",
        "        # 2. Change font colors of all alleles corresponding to each haplotype\n",
        "        # This requires changing colors based on matching the 'MX' (e.g., 'M1') keys from mcm_haplotype_formats\n",
        "        # first extract all values from nested dictionaries containi\n",
        "        # from https://stackoverflow.com/questions/5164642/python-print-a-generator-expression\n",
        "        def NestedDictValues(d):\n",
        "          for v in d.values():\n",
        "            if isinstance(v, dict):\n",
        "              yield from NestedDictValues(v)\n",
        "            else:\n",
        "              yield v\n",
        "\n",
        "        # save haplotype list values\n",
        "        haplotype_value_list = (list(NestedDictValues(mcm)))\n",
        "\n",
        "        # create conditional formats\n",
        "        # note that this will only apply to first 2000 rows\n",
        "        # this should be enough but using much larger values (e.g., 9999) caused Excel to break\n",
        "        for i in haplotype_value_list:\n",
        "            if 'Mafa' not in i: # ignore first list value\n",
        "                for j in i: # unpack list of diagnostic genotypes for each haplotype\n",
        "                    for key, value in mcm_haplotype_formats.items(): # create formats for each haplotype\n",
        "                            if key in j: # only create highlights for genotypes that correspond to haplotypes\n",
        "                                diagnostic_highlight_format = workbook.add_format({'bg_color': value[0], 'font_color': value[1]})\n",
        "\n",
        "                                worksheet.conditional_format('A23:A2000', {'type': 'text',\n",
        "                                                             'criteria': 'containing',\n",
        "                                                             'value': j,\n",
        "                                                             'format': diagnostic_highlight_format,\n",
        "                                                             'stop_if_true' : True})\n",
        "\n",
        "        for key, value in mcm_haplotype_formats.items():\n",
        "            if key == 'M5': #use secondary, darker color for M5\n",
        "                genotype_highlight_format = workbook.add_format({'font_color': value[2]})\n",
        "            else:\n",
        "                genotype_highlight_format = workbook.add_format({'font_color': value[0]})\n",
        "\n",
        "                worksheet.conditional_format('A23:A2000', {'type': 'text',\n",
        "                         'criteria': 'containing',\n",
        "                         'value': key,\n",
        "                         'format': genotype_highlight_format})\n",
        "\n",
        "    ## write headers  - per sample information above data ##\n",
        "\n",
        "    # write header identifiers to first column\n",
        "    header_id =['gs_id',\n",
        "    'client_id',\n",
        "    'mapped_read_count',\n",
        "    'total_read_count',\n",
        "    'percent_reads_unmapped',\n",
        "    'MHC-A Haplotype 1',\n",
        "    'MHC-A Haplotype 2',\n",
        "    'MHC-B Haplotype 1',\n",
        "    'MHC-B Haplotype 2',\n",
        "    'MHC-DRB Haplotype 1',\n",
        "    'MHC-DRB Haplotype 2',\n",
        "    'MHC-DQA Haplotype 1',\n",
        "    'MHC-DQA Haplotype 2',\n",
        "    'MHC-DQB Haplotype 1',\n",
        "    'MHC-DQB Haplodtype 2',\n",
        "    'MHC-DPA Haplotype 1',\n",
        "    'MHC-DPA Haplotype 2',\n",
        "    'MHC-DPB Haplotype 1',\n",
        "    'MHC-DPB Haplotype 2',\n",
        "    'Comments',\n",
        "    'experiment',\n",
        "    'run_id']\n",
        "\n",
        "    # write header id column\n",
        "\n",
        "    for row, i in enumerate(header_id):\n",
        "         worksheet.write(row, 0, i, header_format)\n",
        "\n",
        "    # write header columns\n",
        "\n",
        "    for col, k in enumerate(df_header): # loop over header columns\n",
        "\n",
        "        # ignore allele_group column (first columnn in df_header) to put allele_group data underneath other header identifiers\n",
        "        if col > 0:\n",
        "            worksheet.write(0, col , k[0], header_format) # gs_id\n",
        "            worksheet.write(1, col , k[1], header_format) # client_id\n",
        "            worksheet.write(2, col , k[2], header_number_format) # mapped_read_count\n",
        "            worksheet.write(3, col , k[3], header_number_format) # total_read_count\n",
        "            worksheet.write(4, col , k[4], header_format) # percent_reads_unmapped\n",
        "            worksheet.write(5, col , k[5], header_format) # MHC-A Haplotype 1\n",
        "            worksheet.write(6, col , k[6], header_format) # MHC-A Haplotype 2\n",
        "            worksheet.write(7, col , k[7], header_format) # MHC-B Haplotype 1\n",
        "            worksheet.write(8, col , k[8], header_format) # MHC-B Haplotype 2\n",
        "            worksheet.write(9, col , k[9], header_format) # MHC-DRB Haplotype 1\n",
        "            worksheet.write(10, col , k[10], header_format) # MHC-DRB Haplotype 2\n",
        "            worksheet.write(11, col , k[11], header_format) # MHC-DQA Haplotype 1\n",
        "            worksheet.write(12, col , k[12], header_format) # MHC-DQA Haplotype 2\n",
        "            worksheet.write(13, col , k[13], header_format) # MHC-DQB Haplotype 1\n",
        "            worksheet.write(14, col , k[14], header_format) # MHC-DQB Haplotype 2\n",
        "            worksheet.write(15, col , k[15], header_format) # MHC-DPA Haplotype 1\n",
        "            worksheet.write(16, col , k[16], header_format) # MHC-DPA Haplotype 2\n",
        "            worksheet.write(17, col , k[17], header_format) # MHC-DPB Haplotype 1\n",
        "            worksheet.write(18, col , k[18], header_format) # MHC-DPB Haplotype 2\n",
        "            worksheet.write(19, col , k[19], header_format) # Comments\n",
        "            worksheet.write(20, col , k[20], header_format) # experiment\n",
        "            worksheet.write(21, col , k[21], header_format) # run_id\n",
        "\n",
        "    ## write data ##\n",
        "\n",
        "    # remove NaN values from dataframe\n",
        "    remove_nan = df.replace(np.nan, '', regex=True)\n",
        "\n",
        "    # iterate over each dataframe column\n",
        "    for current_column, column in enumerate(remove_nan):\n",
        "        data = (remove_nan[column].tolist()) # convert dataframe column to list\n",
        "\n",
        "        # iterate over each row and write data\n",
        "        for idx, i in enumerate(data):\n",
        "\n",
        "            # start printing data at row 23 in excel file\n",
        "            starting_row = 22 # 0-based\n",
        "            current_row = idx + starting_row\n",
        "            worksheet.write(current_row, current_column , i)\n",
        "\n",
        "    workbook.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JQGUy0kKKD4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei3PWIK53FIA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz7uv8su3FIA"
      },
      "source": [
        "# Workflow\n",
        "\n",
        "This is the data processing workflow that specifies that order of the commands. It also copies files from the analysis to the output folder so there is a bundle of all the files necessary to reprecreate the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L9fSJZgWoEW"
      },
      "source": [
        "## Get meta data from SampleSheet.csv file\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcnRShnbtnPT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRKFEM_BljWg"
      },
      "source": [
        "## Download files from labkey to google share drive as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkkC-kCassuK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP0BrTSq-UVB"
      },
      "outputs": [],
      "source": [
        "# Sample_ID\tSample_Name\tSample_Project\tDescription\tSpecies\n",
        "import shutil\n",
        "import subprocess\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "lk_species_dict = {'Pig-tailed':'MANE',\n",
        "                   'Cynomolgus':'MCM',\n",
        "                   'Rhesus':'MAMU'}\n",
        "lk_species_list = list(lk_species_dict.keys())\n",
        "\n",
        "df_samples = pd.read_csv(sample_path)\n",
        "\n",
        "# strip leading and trailing whitespace from all columns that are text\n",
        "df_samples['Sample_Name'] = df_samples['Sample_Name'].str.strip()\n",
        "df_samples['Sample_Project'] = df_samples['Sample_Project'].str.strip()\n",
        "df_samples['Description'] = df_samples['Description'].str.strip()\n",
        "df_samples['Species'] = df_samples['Species'].str.strip()\n",
        "\n",
        "if len(project_list)>0:\n",
        "    df_samples = df_samples[df_samples['Sample_Project'].isin(project_list)]\n",
        "print(df_samples)\n",
        "\n",
        "# This is where it gets the sample name from the sheet and renames it to merge the species at project to keep different Species, separate\n",
        "df_samples['PROJECT_NAME'] = ['{0}_{1}'.format(y, lk_species_dict[x]) for x, y in zip(df_samples['Species'], df_samples['Sample_Project'])]\n",
        "\n",
        "# This is the path to the reference this file will use\n",
        "df_samples['REF_PATH'] =  [ref_dict['REF'][lk_species_dict[x]] for x in df_samples['Species']]\n",
        "# This is the path to the primers this file will use it is currently hardcoded to use the same primers for all samples and species.\n",
        "df_samples['PRIMER_PATH'] =  ref_dict['PCR_PRIMERS']\n",
        "#\n",
        "local_filepath = os.path.join('/content', os.path.basename(input_folder))\n",
        "os.makedirs(local_filepath, exist_ok=True)\n",
        "folder_name = os.path.basename(input_folder)\n",
        "rsync_files(source=input_folder,   dest=os.path.dirname(local_filepath))\n",
        "samplepath_base = os.path.basename(sample_path)\n",
        "samplepath_base = samplepath_base[:-4]\n",
        "file_list = os.listdir(local_filepath)\n",
        "file_list.sort()\n",
        "# Get the lis of fastq.gz (or fq.gz) files\n",
        "fastq_list = [x for x in file_list if (x.endswith('.fastq.gz') or x.endswith('.fq.gz')) and not x.startswith('SampleSheet')]\n",
        "fastq_list.sort()\n",
        "\n",
        "df_file = pd.DataFrame()\n",
        "fastq_dict = {}\n",
        "i = 0\n",
        "# There are a few allowable naming conventions\n",
        "for fastq_i in fastq_list:\n",
        "    print(fastq_i)\n",
        "    if len(fastq_i.split('-')) == 3:\n",
        "        sample_number = fastq_i.split('-')[2].split('.')[0]\n",
        "        new_fastq_path = os.path.join(local_filepath, fastq_i)\n",
        "        print(fastq_i,'first')\n",
        "    elif len(fastq_i.split('-'))==2:\n",
        "        sample_number = fastq_i.split('-')[0]\n",
        "        direction_i = fastq_i.split('-')[1].split('.')[0]\n",
        "        fastq_new = '{0}-{1}-{2}.fastq.gz'.format(samplepath_base,\n",
        "                                                  direction_i,\n",
        "                                                  sample_number)\n",
        "        new_fastq_path = os.path.join(local_filepath, fastq_new)\n",
        "        print(fastq_i,'second')\n",
        "        if not os.path.exists(os.path.join(new_fastq_path)):\n",
        "            os.symlink(os.path.join(local_filepath,fastq_i),os.path.join(new_fastq_path))\n",
        "    else:\n",
        "        # This is the naming convention that we would typically work with and we will convert it to the labkey processed format.\n",
        "        # SampleSheet is the name of the sample sheet it used, (with out the .csv extension)\n",
        "        # [SampleSheet Name]-[R1 or R2]-[Sample_ID].fastq.gz\n",
        "        # Example: SampleSheet-R1-70123.fastq.gz\n",
        "        # direction of the read (R1 or R2)\n",
        "        # sample_num (the file number or Sample_ID, this is a unique id for each sample and is enumerated in labkey)\n",
        "        if len(fastq_i.split('_')) == 5:\n",
        "            samplepath_base = 'SampleSheet'\n",
        "            sample_name = fastq_i.split('_')[0]\n",
        "            df_samples_i = df_samples[df_samples['Sample_Name']==sample_name]\n",
        "            sample_num = list(df_samples_i['Sample_ID'])\n",
        "            if len(sample_num) > 0:\n",
        "                sample_num = sample_num[0]\n",
        "            else:\n",
        "                print(\"wrong_naming_convention\",fastq_i)\n",
        "\n",
        "                continue\n",
        "            direction_i = fastq_i.split('_')[3]\n",
        "            extension_i = 'fastq.gz'\n",
        "            fastq_new = '{0}-{1}-{2}.fastq.gz'.format(samplepath_base,\n",
        "                                                  direction_i,\n",
        "                                                  sample_num)\n",
        "\n",
        "            new_fastq_path = os.path.join(local_filepath, fastq_new)\n",
        "            i=i+1\n",
        "        if not os.path.exists(new_fastq_path):\n",
        "            os.symlink(os.path.join(local_filepath,fastq_i),new_fastq_path)\n",
        "file_list = os.listdir(local_filepath)\n",
        "fastq_list = [x for x in file_list if (x.endswith('.fastq.gz') or x.endswith('.fq.gz')) and (len(x.split('-')) == 3)]\n",
        "fastq_list.sort()\n",
        "\n",
        "df_samples.rename(columns={'Species':'SampleId/species', 'Sample_Project':'SampleId/Sample_Project','Description':'SampleId/description','Sample_Name':'SampleId/library_sample_name'},inplace=True)\n",
        "\n",
        "df_file = pd.DataFrame()\n",
        "for fastq_i in fastq_list:\n",
        "    sample_number = fastq_i.split('-')[2].split('.')[0]\n",
        "    new_fastq_path = os.path.join(local_filepath, fastq_i)\n",
        "    df_file_i = pd.DataFrame({'Sample_ID': [int(sample_number)],\n",
        "                            'FILEPATH': [new_fastq_path]})\n",
        "    df_file = pd.concat([df_file, df_file_i], ignore_index=True)\n",
        "df_samples = df_samples.merge(df_file, on='Sample_ID')\n",
        "df_samples['Run'] = RUN_ID\n",
        "# LF2311_S26_L001_R1_001.fastq.gz LF2311_S26_L001_R1_001.fastq.gz\n",
        "print(i)\n",
        "df_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYRLYqaXlqLy"
      },
      "source": [
        "## Main Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JkxkJgM3FIA"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Workflow\n",
        "print('start Pipeline')\n",
        "\n",
        "now = datetime.now()\n",
        "date_time = now.strftime('%Y_%m_%d__%H_%M')\n",
        "## Setup run pick up all the sample projectes so they are given in separate pivot tables/result folders\n",
        "# It also separate by species if the project has multiple species.\n",
        "project_species_list = list(df_samples['PROJECT_NAME'].unique())\n",
        "\n",
        "for project_i in project_species_list:\n",
        "    # Clear output so  customers don't see the outputs from other customers\n",
        "    clear_output()\n",
        "    print('start Pipeline for {0}'.format(project_i))\n",
        "    df_samples_i = df_samples[df_samples['PROJECT_NAME']==project_i]\n",
        "    REF = list(df_samples_i['REF_PATH'])[0]\n",
        "    PCR_PRIMERS = list(df_samples_i['PRIMER_PATH'])[0]\n",
        "    READS = get_read_dict(df_samples_i)\n",
        "    ### create temporary folder\n",
        "    TMP_DIR = os.path.join('/content/tmp',project_i)\n",
        "    os.makedirs(TMP_DIR, exist_ok=True)\n",
        "\n",
        "    species_name = list(df_samples_i['SampleId/species'])[0]\n",
        "    SPECIES = haplotype_dict[lk_species_dict[species_name]]\n",
        "    print(SPECIES)\n",
        "    print(READS)\n",
        "    ### make dataframe to store all genotypes\n",
        "    ALL_GENOTYPES_DF = pd.DataFrame(columns = ['read_ct',\n",
        "                                            'allele',\n",
        "                                            'gs_id',\n",
        "                                            'mapped_read_count',\n",
        "                                            'total_read_count',\n",
        "                                            'percent_reads_unampped',\n",
        "                                            'client_id',\n",
        "                                            'MHC-A Haplotype 1',\n",
        "                                            'MHC-A Haplotype 2',\n",
        "                                            'MHC-B Haplotype 1',\n",
        "                                            'MHC-B Haplotype 2',\n",
        "                                            'MHC-DRB Haplotype 1',\n",
        "                                            'MHC-DRB Haplotype 2',\n",
        "                                            'MHC-DQA Haplotype 1',\n",
        "                                            'MHC-DQA Haplotype 2',\n",
        "                                            'MHC-DQB Haplotype 1',\n",
        "                                            'MHC-DQB Haplotype 2',\n",
        "                                            'MHC-DPA Haplotype 1',\n",
        "                                            'MHC-DPA Haplotype 2',\n",
        "                                            'MHC-DPB Haplotype 1',\n",
        "                                            'MHC-DPB Haplotype 2',\n",
        "                                            'Comments',\n",
        "                                            'experiment',\n",
        "                                            'run_id'\n",
        "                                            ])\n",
        "\n",
        "    ### Create folder for output files\n",
        "\n",
        "    DRIVE_OUTPUT_FOLDER = os.path.join(genotyper_root_dir, 'output',folder_name, '{0}_{1}'.format(project_i, date_time))\n",
        "    OUTPUT_FOLDER = create_output_folder(os.path.join('/content', project_i, 'output') + '/')\n",
        "    if not os.path.exists(OUTPUT_FOLDER + '/SAM'):\n",
        "        os.makedirs(OUTPUT_FOLDER + '/SAM')\n",
        "        os.makedirs(OUTPUT_FOLDER + '/ref')\n",
        "        os.makedirs(OUTPUT_FOLDER + '/dev')\n",
        "\n",
        "    #### Copy reference files to output directory\n",
        "    shutil.copy2(PCR_PRIMERS, OUTPUT_FOLDER + '/ref')\n",
        "    shutil.copy2(REF, OUTPUT_FOLDER + '/ref')\n",
        "\n",
        "    ## Genotype individual samples\n",
        "\n",
        "    ct = 1 # set counter\n",
        "\n",
        "    # create a list to store sample IDs that were skipped\n",
        "    skipped_samples = []\n",
        "\n",
        "    for sample_id, reads in READS.items():\n",
        "\n",
        "            print('--Processing sample ' + str(ct) + ' of ' + str(len(READS)) + '--')\n",
        "            print('Sample ID: ' + sample_id)\n",
        "            print('R1_FASTQ: ' + reads[0])\n",
        "            print('R2_FASTQ: ' + reads[1])\n",
        "            ct = ct + 1\n",
        "\n",
        "            ### trim primers\n",
        "            PRIMER_TRIMMED = remove_primers(R1=reads[0], R2=reads[1], primers=PCR_PRIMERS, out_dir=TMP_DIR)\n",
        "            if not os.path.exists(PRIMER_TRIMMED[0]) or not os.path.exists(PRIMER_TRIMMED[1]):\n",
        "                print(f\"{sample_id} failed primer trimming. Proceeding to next sample.\")\n",
        "                skipped_samples.append(sample_id)\n",
        "                continue\n",
        "\n",
        "            ### merge reads\n",
        "            MERGED = merge_reads(R1=PRIMER_TRIMMED[0], R2=PRIMER_TRIMMED[1], out_dir=TMP_DIR)\n",
        "            if file_size(MERGED) == 0:\n",
        "                print(sample_id + ' merged FASTQ file is empty')\n",
        "                skipped_samples.append(sample_id)\n",
        "                continue\n",
        "\n",
        "            ### count merged reads\n",
        "            TOTAL_READ_CT = count_reads(MERGED)\n",
        "\n",
        "            ### cluster identical reads\n",
        "            UNIQUE = vsearch_unique(reads=MERGED, out_dir=TMP_DIR)\n",
        "            if file_size(UNIQUE) == 0:\n",
        "                print(sample_id + ' unique FASTQ file is empty')\n",
        "                skipped_samples.append(sample_id)\n",
        "                continue\n",
        "\n",
        "            ## map reads and save potential full-length reads\n",
        "            ## this avoids partial-matches obscuring full-length matches following UNOISE3\n",
        "            REMOVE_PARTIAL = map_semiperfect(reads=UNIQUE, ref=REF, out_dir=TMP_DIR, out_fmt='fasta')\n",
        "            if file_size(REMOVE_PARTIAL) == 0:\n",
        "                print(sample_id + ' mapped FASTA file is empty')\n",
        "                skipped_samples.append(sample_id)\n",
        "                continue\n",
        "\n",
        "            ### denoise artifacts and chimeric sequences\n",
        "            ZOTU = vsearch_denoise(reads=REMOVE_PARTIAL, read_ct=TOTAL_READ_CT, out_dir=TMP_DIR)\n",
        "            if file_size(ZOTU) == 0:\n",
        "                print(sample_id + ' denoised FASTQ file is empty')\n",
        "                skipped_samples.append(sample_id)\n",
        "                continue\n",
        "\n",
        "            ### map reads and copy SAM to output folder\n",
        "            MAPPED_SAM = map_semiperfect(reads=ZOTU, ref=REF, out_dir=TMP_DIR, out_fmt='sam')\n",
        "            if file_size(MAPPED_SAM) == 0:\n",
        "                print(sample_id + ' mapped SAM file is empty')\n",
        "                skipped_samples.append(sample_id)\n",
        "                continue\n",
        "\n",
        "            shutil.copy2(MAPPED_SAM, OUTPUT_FOLDER + '/SAM')\n",
        "\n",
        "            ### parse sam\n",
        "            # reads[2] is client_id extracted from LabKey\n",
        "            GENOTYPES = parse_sam(MAPPED_SAM, sample_id, TOTAL_READ_CT, reads[2], EXPERIMENT, reads[3], SPECIES)\n",
        "\n",
        "            ### append genotypes to all_genotypes_df\n",
        "            ALL_GENOTYPES_DF = pd.concat([ALL_GENOTYPES_DF, GENOTYPES])\n",
        "            print(ct)\n",
        "\n",
        "\n",
        "    # print any samples that were skipped\n",
        "    print(f\"{len(skipped_samples)} samples were skipped:\")\n",
        "    print(\", \".join(skipped_samples))\n",
        "\n",
        "    ## Copy developer files to output folder - eventually make a flag to only display if necessary\n",
        "    ALL_GENOTYPES_DF.to_csv(OUTPUT_FOLDER + '/dev/genotypes.csv')\n",
        "    # ALL_GENOTYPES_DF.to_csv(DRIVE_OUTPUT_FOLDER + '/dev/genotypes.csv')\n",
        "\n",
        "    ## Report genotypes\n",
        "\n",
        "    ### Make pivot table in pandas\n",
        "    if len(ALL_GENOTYPES_DF) > 0:\n",
        "        GENOTYPES_PIVOTED = pivot_pandas(df = ALL_GENOTYPES_DF )\n",
        "\n",
        "    ### Write Excel report\n",
        "        generate_excel_report(df = GENOTYPES_PIVOTED, out_dir = OUTPUT_FOLDER)\n",
        "    else:\n",
        "        print('All samples failed generate enough reads, no pivot created')\n",
        "        Path(os.path.join(OUTPUT_FOLDER,'pivot_is_blank.txt')).touch()\n",
        "    # Bake up files\n",
        "    os.makedirs(DRIVE_OUTPUT_FOLDER, exist_ok=True)\n",
        "    rsync_files(source=os.path.join(Shareddrives_path,'dholab/gs/genotyper/27052-miseq-genotyping.ipynb'),\n",
        "                dest='{0}/27052-miseq-genotyping.ipynb'.format(DRIVE_OUTPUT_FOLDER))\n",
        "    rsync_files(source='{0}/'.format(OUTPUT_FOLDER),\n",
        "                dest='{0}/'.format(DRIVE_OUTPUT_FOLDER))\n",
        "    print('--PIPELINE COMPLETE--')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "1148px",
        "left": "2045.3492431640625px",
        "right": "20.5330867767334px",
        "top": "119.98161315917969px",
        "width": "800px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}